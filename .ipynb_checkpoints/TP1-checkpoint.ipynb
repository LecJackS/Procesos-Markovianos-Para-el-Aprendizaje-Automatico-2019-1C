{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8NxgP4gIN5eF"
   },
   "source": [
    "**Recycling robot example** (from Sutton, page 42)\n",
    "References:\n",
    "  - Gym documentation: https://gym.openai.com/\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fQ-0sEtFFcTM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym.envs.toy_text import discrete\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i3dzvy9s6aX3"
   },
   "source": [
    "##### TODO: Describir coloquialmente el modelo de sutton\n",
    "Dos estados: high y low\n",
    "Tres acciones: search, wait, recharge\n",
    "\n",
    "##### TODO: Explicar lo básico de GYM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPaZiYtu6aX6"
   },
   "source": [
    "# Considere el modelo del robot de reciclaje descríto en Sutton Example 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/example3.2-1.png\" width=\"500\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/example3.2-2.png\" width=\"500\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U96qJdswGBFr"
   },
   "outputs": [],
   "source": [
    "states = [\"high\", \"low\"]\n",
    "actions = [\"wait\", \"search\", \"recharge\"]\n",
    "\n",
    "P = {}\n",
    "\n",
    "P[0] = {}\n",
    "P[1] = {}\n",
    "\n",
    "alpha = 0.8\n",
    "beta = 0.1\n",
    "r_wait = 0.5\n",
    "r_search = 2.0\n",
    "\n",
    "# definimos un ambiente discreto con las transiciones según el gráfico\n",
    "def generar_ambiente(alpha=alpha, beta=beta, r_wait=r_wait, r_search=r_search):\n",
    "    P[0][0] = [(1.0, 0, r_wait, False)]\n",
    "    P[0][1] = [(alpha, 0, r_search, False),\n",
    "               (1-alpha, 1, r_search, False)]\n",
    "    P[0][2] = [(1,0,0,False)]\n",
    "\n",
    "    P[1][0] = [(1.0, 1, r_wait, False)]\n",
    "    P[1][1] = [(beta, 1, r_search, False), \n",
    "               (1-beta, 0, -3.0, False)]\n",
    "    P[1][2] = [(1.0, 0, 0.0, False)]\n",
    "    env = discrete.DiscreteEnv(2, 3, P, [0.0, 1.0])\n",
    "    return(env)\n",
    "env = generar_ambiente()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvcF7--Z6aX8"
   },
   "source": [
    "# Implemente la estrategia random para veinte episodios. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XoNiKgvOIC3n"
   },
   "source": [
    "Definir una acción aleatoria y ver que reward produce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs\tDone\tReward\tC.Reward\tAction\n",
      "1 \t False \t 0.5 \t 0.500 \t\t 0\n",
      "1 \t False \t 0.5 \t 1.000 \t\t 0\n",
      "1 \t False \t 0.5 \t 1.500 \t\t 0\n",
      "0 \t False \t 0.0 \t 1.500 \t\t 2\n",
      "0 \t False \t 0.5 \t 2.000 \t\t 0\n",
      "0 \t False \t 0.5 \t 2.500 \t\t 0\n",
      "0 \t False \t 0.5 \t 3.000 \t\t 0\n",
      "0 \t False \t 2.0 \t 5.000 \t\t 1\n",
      "0 \t False \t 0.5 \t 5.500 \t\t 0\n",
      "0 \t False \t 2.0 \t 7.500 \t\t 1\n",
      "0 \t False \t 0.5 \t 8.000 \t\t 0\n",
      "0 \t False \t 2.0 \t 10.000 \t\t 1\n",
      "0 \t False \t 2.0 \t 12.000 \t\t 1\n",
      "0 \t False \t 2.0 \t 14.000 \t\t 1\n",
      "0 \t False \t 2.0 \t 16.000 \t\t 1\n",
      "0 \t False \t 0.5 \t 16.500 \t\t 0\n",
      "0 \t False \t 2.0 \t 18.500 \t\t 1\n",
      "0 \t False \t 0.5 \t 19.000 \t\t 0\n",
      "0 \t False \t 0.5 \t 19.500 \t\t 0\n",
      "0 \t False \t 2.0 \t 21.500 \t\t 1\n",
      "0 \t False \t 0.5 \t 22.000 \t\t 0\n",
      "0 \t False \t 0.5 \t 22.500 \t\t 0\n",
      "0 \t False \t 0.5 \t 23.000 \t\t 0\n",
      "0 \t False \t 2.0 \t 25.000 \t\t 1\n",
      "0 \t False \t 0.5 \t 25.500 \t\t 0\n",
      "0 \t False \t 2.0 \t 27.500 \t\t 1\n",
      "0 \t False \t 0.5 \t 28.000 \t\t 0\n",
      "0 \t False \t 0.5 \t 28.500 \t\t 0\n",
      "0 \t False \t 2.0 \t 30.500 \t\t 1\n",
      "0 \t False \t 2.0 \t 32.500 \t\t 1\n",
      "0 \t False \t 0.5 \t 33.000 \t\t 0\n",
      "0 \t False \t 2.0 \t 35.000 \t\t 1\n",
      "0 \t False \t 0.5 \t 35.500 \t\t 0\n",
      "1 \t False \t 2.0 \t 37.500 \t\t 1\n",
      "0 \t False \t 0.0 \t 37.500 \t\t 2\n",
      "0 \t False \t 2.0 \t 39.500 \t\t 1\n",
      "0 \t False \t 0.5 \t 40.000 \t\t 0\n",
      "0 \t False \t 2.0 \t 42.000 \t\t 1\n",
      "1 \t False \t 2.0 \t 44.000 \t\t 1\n",
      "0 \t False \t 0.0 \t 44.000 \t\t 2\n",
      "0 \t False \t 2.0 \t 46.000 \t\t 1\n",
      "0 \t False \t 0.5 \t 46.500 \t\t 0\n",
      "1 \t False \t 2.0 \t 48.500 \t\t 1\n",
      "0 \t False \t 0.0 \t 48.500 \t\t 2\n",
      "0 \t False \t 0.5 \t 49.000 \t\t 0\n",
      "0 \t False \t 0.5 \t 49.500 \t\t 0\n",
      "0 \t False \t 2.0 \t 51.500 \t\t 1\n",
      "0 \t False \t 0.5 \t 52.000 \t\t 0\n",
      "0 \t False \t 0.5 \t 52.500 \t\t 0\n",
      "0 \t False \t 0.5 \t 53.000 \t\t 0\n"
     ]
    }
   ],
   "source": [
    "env = generar_ambiente()\n",
    "print(\"Obs\\tDone\\tReward\\tC.Reward\\tAction\")\n",
    "verbose=True\n",
    "history = []\n",
    "rewardAcum = 0\n",
    "state=1 # < starting state\n",
    "for i in range(50):\n",
    "    #action = env.action_space.sample()\n",
    "    if state == 1:\n",
    "        action = np.random.choice([0,1,2])\n",
    "    else:\n",
    "        action = np.random.choice([0,1])\n",
    "    #print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "    rewardAcum += reward\n",
    "    elemHist = np.array([i, reward, rewardAcum])\n",
    "    history.append(elemHist)\n",
    "    if verbose:\n",
    "        print(state,\"\\t\", done,\"\\t\", reward, \"\\t\", \"%.3f\" % rewardAcum, \"\\t\\t\", action)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9kpyAEk6aYB"
   },
   "source": [
    "# Grafique la recompensa acumulada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.5, 0.5],\n",
       "       [1. , 0.5, 1. ],\n",
       "       [2. , 0.5, 1.5],\n",
       "       [3. , 0. , 1.5],\n",
       "       [4. , 0.5, 2. ],\n",
       "       [5. , 0.5, 2.5]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to numpy\n",
    "history = np.array(history)\n",
    "history[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 0.5, 0. , 0.5, 0.5])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so I can do this\n",
    "history[0:6, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fab03f73ef0>"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fab04063ef0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.grid()\n",
    "plt.xlabel(\"time step\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.xticks(np.arange(0,len(history),1))\n",
    "plt.plot(history[:,0], history[:,2], 'ro', label='Reward acumulado')\n",
    "plt.plot(history[:,0], history[:,1], 'b.', label='Reward instantaneo')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qC0PJOkV6aYH"
   },
   "source": [
    "# Calcule de forma teórica V, la value function optima para cada estado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**PREGUNTAR v !!!**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #x: actual state\n",
    "    #y's: next states\n",
    "\n",
    "    V(x) = reward + gamma (sum_y( p(y|a,x) * V(y) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V^*(x) = R + \\gamma * \\sum_{y \\in S'}\\ p(y\\ |\\ a,x) * V(y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZRz0uMX1LbXz"
   },
   "source": [
    "# Implemente el algoritmo de iteración de valor (Value iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/value-iteration-4.4.png\" width=\"500\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVuBLOrWL1ru"
   },
   "source": [
    "  Evaluate the optimal value function given a full description of the environment dynamics\n",
    "  \n",
    "  \n",
    "\n",
    "```\n",
    " Args:\n",
    "\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "  \n",
    "  Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "```\n",
    "\n",
    "\n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: 0 , action: 0  ->  [(1.0, 0, 0.5, False)]\n",
      "state: 0 , action: 1  ->  [(0.8, 0, 2.0, False), (0.19999999999999996, 1, 2.0, False)]\n",
      "state: 0 , action: 2  ->  [(1, 0, 0, False)]\n",
      "state: 1 , action: 0  ->  [(1.0, 1, 0.5, False)]\n",
      "state: 1 , action: 1  ->  [(0.1, 1, 2.0, False), (0.9, 0, -3.0, False)]\n",
      "state: 1 , action: 2  ->  [(1.0, 0, 0.0, False)]\n"
     ]
    }
   ],
   "source": [
    "for s in range(env.nS):\n",
    "    for a in range(env.nA):\n",
    "        print(\"state:\",s, \", action:\",a, \" -> \" ,env.P[s][a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en cada estado, quiero calcular la value function optima;\n",
    "# uso ecuacion de bellman, tomando el reward instantaneo y\n",
    "# estimando futuros posibles rewards a partir de las acciones.\n",
    "def valueIteration(env, theta=0.1, gamma=0.9, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"epoch\\tvalue V^pi_i\")\n",
    "    update=float('inf')\n",
    "    #V = np.zeros((env.nS, env.nA))\n",
    "    V = np.zeros(env.nS)\n",
    "    policyPi = np.zeros((env.nS, env.nA))\n",
    "    epoch=0\n",
    "    while(update > theta):\n",
    "        # bellman eq, for each state\n",
    "        for s in range(env.nS):\n",
    "            vPrev = V[s]\n",
    "            maxReward = -float('inf')\n",
    "            for a in range(env.nA):\n",
    "                sumFutureRewards = 0\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    #sum rewards over all possible next states\n",
    "                    sumFutureRewards += prob * (reward + gamma*V[next_state])\n",
    "\n",
    "                # me quedo con el mayor reward entre todas las actions a posibles en\n",
    "                # el estado s\n",
    "                if(maxReward < sumFutureRewards):\n",
    "                    maxReward = sumFutureRewards\n",
    "                    maxAction = a\n",
    "\n",
    "            # me comporto greedy al elegir el valor maximo entre los obtenidos \n",
    "            V[s] = maxReward\n",
    "            policyPi[s][maxAction] = 1\n",
    "            update = abs(V[s] - vPrev)\n",
    "            #print(update)\n",
    "        epoch += 1\n",
    "        if verbose:\n",
    "            print(epoch, \":\", V)\n",
    "    print(\"Value Function after\", epoch, \"epochs:\",V)\n",
    "    return policyPi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function after 25 epochs: [16.21487099 14.59338389]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valueIteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KGWxWGNA6aYJ"
   },
   "source": [
    "# Implemente el algoritmo de policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f9o3S2kaKQKX"
   },
   "source": [
    "Definir primero una funcion de evaluación de politica,\n",
    "\n",
    "```\n",
    "Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "        \n",
    "```\n",
    "\n",
    "Despues una funcion de optimisacion de la politica:\n",
    "\n",
    "\n",
    "```\n",
    " Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/policy-evaluation-4.1.png\" width=\"500\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/value-vs-policy.png\" width=\"500\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Evaluation \n",
    "# copypasteo la de value iteration:\n",
    "# en cada estado, quiero calcular la value function PARA UNA POLICY PI;\n",
    "# uso ecuacion de bellman, tomando el reward instantaneo y\n",
    "# estimando futuros posibles rewards a partir de las acciones, que son elegidas\n",
    "# a partir de la policy pi.\n",
    "def policyEvaluation(env, policy, theta=0.1, gamma=0.9, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"epoch\\tvalue V^pi_i\")\n",
    "    update=float('inf')\n",
    "    \n",
    "    #V = np.zeros((env.nS, env.nA))\n",
    "\n",
    "    V = np.zeros(env.nS)\n",
    "    epoch=0\n",
    "    while(update > theta):\n",
    "        # bellman eq, for each state\n",
    "        for s in range(env.nS):\n",
    "            vPrev = V[s]\n",
    "            piReward = -float('inf')\n",
    "            #for a in policy[s,:]:\n",
    "            #sumFutureRewards = 0\n",
    "            piReward=0\n",
    "            a = np.random.choice([0,1,2], p=policy[s])\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                # cada una de las acciones que realiza la policy en el estado s\n",
    "                # sums rewards ONLY OVER policy's actions\n",
    "                #sumFutureRewards += prob * (reward + gamma*V[next_state])\n",
    "                piReward += prob * (reward + gamma*V[next_state])\n",
    "                # me quedo con el mayor reward entre todas las actions a posibles en\n",
    "                # el estado s\n",
    "                #maxReward = max(maxReward, sumFutureRewards)\n",
    "                #piReward += sumFutureRewards\n",
    "            # me comporto greedy al elegir el valor maximo entre los obtenidos \n",
    "            V[s] = piReward\n",
    "            update = abs(V[s] - vPrev)\n",
    "            #print(update)\n",
    "        epoch += 1\n",
    "        if verbose:\n",
    "            print(epoch, \":\", V)\n",
    "    print(\"Value Function of policy Pi after\", epoch, \"epochs:\",V)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\tvalue V^pi_i\n",
      "1 : [2.  0.5]\n",
      "2 : [3.53 0.95]\n",
      "3 : [4.7126 1.355 ]\n",
      "4 : [5.636972 1.7195  ]\n",
      "5 : [6.36812984 2.04755   ]\n",
      "6 : [6.95361248 2.342795  ]\n",
      "7 : [7.42830409 2.6085155 ]\n",
      "8 : [7.81791173 2.84766395]\n",
      "9 : [8.14147596 3.06289756]\n",
      "10 : [8.41318425 3.2566078 ]\n",
      "11 : [8.64368206 3.43094702]\n",
      "12 : [8.84102155 3.58785232]\n",
      "13 : [9.01134893 3.72906709]\n",
      "14 : [9.15940331 3.85616038]\n",
      "15 : [9.28887925 3.97054434]\n",
      "16 : [9.40269104 4.07348991]\n",
      "17 : [9.50316573 4.16614092]\n",
      "Value Function of policy Pi after 17 epochs: [9.50316573 4.16614092]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9.50316573, 4.16614092])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy=np.zeros((env.nS, env.nA))#[S,A]\n",
    "#le doy dos valores para que la suma ed las probas ed 1 y no\n",
    "# explote el random.choice([], p=[probas])\n",
    "policy[0][1] = 1\n",
    "policy[1][0] = 1\n",
    "policyEvaluation(env, policy, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/policy-iteration-4.3.png\" width=\"500\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/value-iteration-4.4.png\" width=\"500\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policyPi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporal!!\n",
    "def valueSearch(env, theta=0.1, gamma=0.9, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"epoch\\tvalue V^pi_i\")\n",
    "    update=float('inf')\n",
    "    #V = np.zeros((env.nS, env.nA))\n",
    "    V = np.zeros(env.nS)\n",
    "    policyPi = np.zeros((env.nS, env.nA))\n",
    "    epoch=0\n",
    "    while(update > theta):\n",
    "        # bellman eq, for each state\n",
    "        for s in range(env.nS):\n",
    "            vPrev = V[s]\n",
    "            maxReward = -float('inf')\n",
    "            for a in range(env.nA):\n",
    "                sumFutureRewards = 0\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    #sum rewards over all possible next states\n",
    "                    sumFutureRewards += prob * (reward + gamma*V[next_state])\n",
    "\n",
    "                # me quedo con el mayor reward entre todas las actions a posibles en\n",
    "                # el estado s\n",
    "                if(maxReward < sumFutureRewards):\n",
    "                    maxReward = sumFutureRewards\n",
    "                    maxAction = a\n",
    "\n",
    "            # me comporto greedy al elegir el valor maximo entre los obtenidos \n",
    "            V[s] = maxReward\n",
    "            policyPi[s][maxAction] = 1\n",
    "            update = abs(V[s] - vPrev)\n",
    "            #print(update)\n",
    "        epoch += 1\n",
    "        if verbose:\n",
    "            print(epoch, \":\", V)\n",
    "    print(\"Value Function after\", epoch, \"epochs:\",V)\n",
    "    return policyPi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyImprovement(env, policy, gamma=0.9):\n",
    "    print(policy)\n",
    "    policyStable = False\n",
    "    V = policyEvaluation(env, policy, gamma=0.9, verbose=False)\n",
    "    while(not policyStable):\n",
    "        policyStable = True\n",
    "        for s in range(env.nS):\n",
    "            print(\"For state s:\", s)\n",
    "            #oldA = np.random.choice(policy[s], policy[s])\n",
    "            oldA = np.array(policy[s])\n",
    "            \n",
    "            # action siguiendo policy Pi en state s\n",
    "            #if state == 1:\n",
    "            #    a = np.random.choice([0,1,2], p=policy[s])\n",
    "            #else:\n",
    "            #    a = np.random.choice([0,1,2], p=policy[s])\n",
    "            \n",
    "            #print(maxReward, sumFutureRewards)\n",
    "            maxReward = -float('inf')\n",
    "            maxAction = 0;\n",
    "            for a in range(env.nA):\n",
    "                sumFutureRewards = 0\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    #sum rewards over all possible next states\n",
    "                    sumFutureRewards += prob * (reward + gamma*V[next_state])\n",
    "                #print(maxReward, sumFutureRewards)\n",
    "\n",
    "                # me quedo con el mayor reward entre todas las actions a posibles en\n",
    "                # el estado s\n",
    "                if(maxReward < sumFutureRewards):\n",
    "                    maxReward = sumFutureRewards\n",
    "                    maxAction = a\n",
    "            #policyPi = valueIteration(env, gamma=0.9, verbose=False)\n",
    "            oneHot = np.zeros(3)\n",
    "            oneHot[maxAction] = 1\n",
    "            policy[s] = oneHot\n",
    "            #if oldA != policy[s]:\n",
    "            #print(oldA, policy[s])\n",
    "            if not np.array_equal(oldA, policy[s]):\n",
    "                print(\"policyStable to:\", policyStable)\n",
    "                policyStable = False\n",
    "        if policyStable:\n",
    "            return (policy, V)\n",
    "        else:\n",
    "            print(policy)\n",
    "            V = policyEvaluation(env, policy, gamma=0.9, verbose=False)\n",
    "            #como lo comparo? que mido?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el algoritmo corta antes cuando llega algo estable, eso puede ser que cause qe el valor no sea el optimo\n",
    "# TODO\n",
    "# usar una diferencia ed la misma forma que con la value iteration\n",
    "# los valores deberian ser los mismos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "Value Function of policy Pi after 17 epochs: [4.16614092 4.16614092]\n",
      "For state s: 0\n",
      "policyStable to: True\n",
      "For state s: 1\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "Value Function of policy Pi after 17 epochs: [6.72452818 4.16614092]\n",
      "For state s: 0\n",
      "For state s: 1\n",
      "policyStable to: True\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "Value Function of policy Pi after 21 epochs: [8.36147152 7.52532436]\n",
      "For state s: 0\n",
      "For state s: 1\n",
      "(array([[0., 1., 0.],\n",
      "       [0., 0., 1.]]), array([8.36147152, 7.52532436]))\n"
     ]
    }
   ],
   "source": [
    "policyRand=np.zeros((env.nS, env.nA))#[S,A]\n",
    "#le doy dos valores para que la suma ed las probas ed 1 y no\n",
    "# explote el random.choice([], p=[probas])\n",
    "policyRand[0][0] = 1\n",
    "policyRand[1][0] = 1\n",
    "print(policyRand)\n",
    "print(policyImprovement(env, policy=policyRand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.72452818, 4.16614092])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "brOjUJvE6aYV"
   },
   "source": [
    "# Utilizando los 3 algoritmos, realice los experimentos para las siguientes configuraciones del ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVKvrm0t6aYV"
   },
   "outputs": [],
   "source": [
    "exp1 = generar_ambiente(alpha=0.9, beta=0.9, r_search=3, r_wait=2)\n",
    "exp2 = generar_ambiente(alpha=0.8, beta=0.5, r_search=3, r_wait=2)\n",
    "exp3 = generar_ambiente(alpha=0.5, beta=0.5, r_search=3, r_wait=2)\n",
    "exp4 = generar_ambiente(alpha=0.9, beta=0.6, r_search=1, r_wait=0.9)\n",
    "exp5 = generar_ambiente(alpha=0.9, beta=0.6, r_search=1, r_wait=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAT_YZq56aYY"
   },
   "source": [
    "# Utilizando el grafico de recompensa, compare las estrategias óptimas generadas con los experimentos anteriores contra la estrategia al azar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "TP1 - (Sin solucion).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
