{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliBanditEnv(object): \n",
    "    # Clase que define el environment donde el reward es 0 o 1 dependiendo de una probabilidad p.\n",
    "\n",
    "    def __init__(self, num_arms=10, p=None):\n",
    "        self.num_arms = num_arms\n",
    "        self.actions = np.arange(num_arms)     # define set of actions\n",
    "\n",
    "        if len(p)==1:\n",
    "            self.p = np.random.beta(0.5, 0.5, size=num_arms)\n",
    "        elif len(p) == num_arms:\n",
    "            self.p = p\n",
    "        else:\n",
    "            raise Exception('Number of probabilities ({}) does not correspond to number of arms ({}).'.format(len(q), num_arms))\n",
    "        self.best_action = np.argmax(self.p)   # La mejor accion dado el enviroenment\n",
    "\n",
    "    def reward(self, action):\n",
    "        return np.random.binomial(1, p=self.p[action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    # Clase que define al agente. Cada agente cuenta con una regla de decisión y una regla de aprendizaje.\n",
    "    \n",
    "    def __init__(self, learning_rule, decision_rule, param=None):\n",
    "        self.decision_rule = decision_rule\n",
    "        self.learning_rule = learning_rule\n",
    "\n",
    "        if decision_rule == \"epsilon-greedy\":\n",
    "            self.epsilon = param[\"epsilon\"]\n",
    "        \n",
    "        if decision_rule == \"UCB\":\n",
    "            self.UCB_param = param[\"UCB_param\"]\n",
    "    \n",
    "    def environment(self, env, init_q):\n",
    "        # inicializa el environment\n",
    "        self.env = env                                  \n",
    "        self.k = env.num_arms                           \n",
    "        self.actions = np.arange(self.k)                \n",
    "        self.act_count = np.zeros(self.k)               \n",
    "        self.iteration = 0     \n",
    "        if self.learning_rule == \"BayesianBetaPrior\":\n",
    "            self.alpha = np.random.uniform(size=self.k)\n",
    "            self.beta = np.random.uniform(size=self.k)\n",
    "        if len(init_q) == self.k:\n",
    "            self.q_estimate = init_q\n",
    "        else:\n",
    "            raise Exception('Number of initial values ({}) does not correspond to number of arms ({}).'.format(len(init_q), self.k))\n",
    "\n",
    "    def learn(self, a, r):\n",
    "        # dada una acción y una recompenza, actualiza la value function.\n",
    "        if self.learning_rule == \"averaging\":\n",
    "            self.q_estimate[a] += 1/self.act_count[a] * (r - self.q_estimate[a])\n",
    "            \n",
    "        if self.learning_rule == \"BayesianBetaPrior\":\n",
    "            self.alpha[a] += r\n",
    "            self.beta[a] += 1 - r \n",
    "            \n",
    "    def act(self):\n",
    "        # realiza una acción.\n",
    "        self.iteration += 1 \n",
    "        if self.decision_rule == \"greedy\":\n",
    "            # COMPLETAR\n",
    "            pass\n",
    "\n",
    "        if self.decision_rule == \"epsilon-greedy\":\n",
    "            # COMPLETAR\n",
    "            selected_action = 0\n",
    "            pass\n",
    "        \n",
    "        if self.decision_rule == \"UCB\":\n",
    "            # COMPLETAR\n",
    "            pass\n",
    "        if self.decision_rule == \"Thompson\":\n",
    "            # COMPLETAR\n",
    "            pass\n",
    "        self.act_count[selected_action] += 1\n",
    "        return selected_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateBandits(agents, narms, initp=None, initq=None, repetitions=1000, N=100):\n",
    "    # función que realiza las simulaciones de los agentes. Se define el número de repeticiones que seran\n",
    "    #  promediadas y el número de pasos N. agents es una lista de agentes.\n",
    "    \n",
    "    rewards = np.zeros((len(agents), repetitions, N))\n",
    "    bestarm = np.zeros((len(agents), repetitions, N))\n",
    "    for i, agent in enumerate(agents):\n",
    "        for j in np.arange(repetitions):\n",
    "            environment = BernoulliBanditEnv(num_arms=narms, p=initp)\n",
    "            agent.environment(environment, initq if not(initq == None) else np.zeros(narms))\n",
    "            for n in np.arange(N):\n",
    "                a = agent.act()\n",
    "                r = environment.reward(a)\n",
    "                agent.learn(a, r)\n",
    "                rewards[i, j, n] = r\n",
    "                bestarm[i, j, n] = 1 if a == environment.best_action else 0\n",
    "    \n",
    "    return np.squeeze(np.mean(rewards, axis=1)), np.squeeze(np.mean(bestarm, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(agents, actions, rewards):\n",
    "    # COMPLETAR\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.481, 0.531, 0.505, 0.521, 0.489, 0.504, 0.503, 0.498, 0.502,\n",
      "       0.548, 0.484, 0.508, 0.487, 0.482, 0.471, 0.509, 0.504, 0.504,\n",
      "       0.452, 0.509, 0.468, 0.506, 0.534, 0.497, 0.511, 0.479, 0.518,\n",
      "       0.508, 0.499, 0.488, 0.527, 0.51 , 0.489, 0.473, 0.461, 0.52 ,\n",
      "       0.516, 0.532, 0.49 , 0.506, 0.476, 0.522, 0.534, 0.49 , 0.494,\n",
      "       0.531, 0.504, 0.516, 0.493, 0.51 , 0.488, 0.481, 0.487, 0.507,\n",
      "       0.494, 0.498, 0.494, 0.508, 0.516, 0.482, 0.493, 0.509, 0.455,\n",
      "       0.496, 0.506, 0.502, 0.477, 0.487, 0.535, 0.497, 0.507, 0.505,\n",
      "       0.51 , 0.488, 0.489, 0.507, 0.48 , 0.526, 0.465, 0.484, 0.492,\n",
      "       0.471, 0.491, 0.491, 0.511, 0.491, 0.511, 0.523, 0.508, 0.523,\n",
      "       0.491, 0.475, 0.48 , 0.484, 0.473, 0.52 , 0.497, 0.497, 0.516,\n",
      "       0.475]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))\n"
     ]
    }
   ],
   "source": [
    "# new agent\n",
    "params = {\"epsilon\": 0.1}\n",
    "agent = Agent(\"averaging\", \"epsilon-greedy\", params)\n",
    "agents = [agent]\n",
    "# print simulation with n arms\n",
    "\n",
    "initp = [0.5, 0.5] #prob of something of each arm\n",
    "narms = len(initp)\n",
    "print(simulateBandits(agents, narms=narms, initp=initp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios:\n",
    "\n",
    "1) Completar pertinentemente el código donde diga \"COMPLETAR\".\n",
    "\n",
    "2) Realizar simulaciones con un bandit de 2 brazos (P = [0.4, 0.8]) para cada una de las reglas de decisión y graficar la recompensa promedio, la recompensa acumulada y el porcentaje de veces que fue elegido el mejor brazo en función de los pasos. Interprete los resultados.\n",
    "\n",
    "3) Realizar simulaciones con un bandit de 10 brazos (P = [0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.2]) para cada una de las reglas de decisión y graficar la recompensa promedio, la recompensa acumulada y el porcentaje de veces que fue elegido el mejor brazo en función de los pasos. Interprete los resultados.\n",
    "\n",
    "4) Estudie la dependencia del hiperparametro epsilon en la regla de decisión epsilon-greedy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
