{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8NxgP4gIN5eF"
   },
   "source": [
    "**Recycling robot example** (from Sutton, page 42)\n",
    "References:\n",
    "  - Gym documentation: https://gym.openai.com/\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fQ-0sEtFFcTM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym.envs.toy_text import discrete\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i3dzvy9s6aX3"
   },
   "source": [
    "##### TODO: Describir coloquialmente el modelo de sutton\n",
    "Dos estados: high y low\n",
    "Tres acciones: search, wait, recharge\n",
    "\n",
    "##### TODO: Explicar lo básico de GYM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPaZiYtu6aX6"
   },
   "source": [
    "# Considere el modelo del robot de reciclaje descríto en Sutton Example 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/example3.2-1.png\" width=\"500\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/example3.2-2.png\" width=\"500\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U96qJdswGBFr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jack\\gym\\gym\\__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "states = [\"high\", \"low\"]\n",
    "actions = [\"wait\", \"search\", \"recharge\"]\n",
    "\n",
    "P = {}\n",
    "\n",
    "P[0] = {}\n",
    "P[1] = {}\n",
    "\n",
    "alpha = 0.9\n",
    "beta = 1\n",
    "r_wait = 0.5\n",
    "r_search = 2.0\n",
    "\n",
    "# definimos un ambiente discreto con las transiciones según el gráfico\n",
    "def generar_ambiente(alpha=alpha, beta=beta, r_wait=r_wait, r_search=r_wait):\n",
    "    P[0][0] = [(1.0, 0, r_wait, False)]\n",
    "    P[0][1] = [(alpha, 0, r_search, False),\n",
    "               (1-alpha, 1, r_search, False)]\n",
    "    P[0][2] = [(1,0,0,False)]\n",
    "\n",
    "    P[1][0] = [(1.0, 1, r_wait, False)]\n",
    "    P[1][1] = [(beta, 1, r_search, False), \n",
    "               (1-beta, 0, -3.0, False)]\n",
    "    P[1][2] = [(1.0, 0, 0.0, False)]\n",
    "    env = discrete.DiscreteEnv(2, 3, P, [0.0, 1.0])\n",
    "    return(env)\n",
    "env = generar_ambiente()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvcF7--Z6aX8"
   },
   "source": [
    "# Implemente la estrategia random para veinte episodios. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XoNiKgvOIC3n"
   },
   "source": [
    "Definir una acción aleatoria y ver que reward produce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs\tDone\tReward\tC.Reward\tAction\n",
      "1 \t False \t 0.5 \t 0.500 \t\t 0\n",
      "1 \t False \t 0.5 \t 0.500 \t\t 1\n",
      "1 \t False \t 0.5 \t 0.500 \t\t 0\n",
      "1 \t False \t 0.5 \t 0.500 \t\t 1\n",
      "1 \t False \t 0.5 \t 0.500 \t\t 1\n",
      "0 \t False \t 0.0 \t 0.417 \t\t 2\n",
      "0 \t False \t 0.5 \t 0.429 \t\t 0\n",
      "0 \t False \t 0 \t 0.375 \t\t 2\n",
      "0 \t False \t 0.5 \t 0.389 \t\t 0\n",
      "0 \t False \t 0.5 \t 0.400 \t\t 0\n",
      "0 \t False \t 0.5 \t 0.409 \t\t 0\n",
      "0 \t False \t 0 \t 0.375 \t\t 2\n",
      "0 \t False \t 0.5 \t 0.385 \t\t 1\n",
      "0 \t False \t 0 \t 0.357 \t\t 2\n",
      "0 \t False \t 0 \t 0.333 \t\t 2\n",
      "0 \t False \t 0.5 \t 0.344 \t\t 0\n",
      "0 \t False \t 0.5 \t 0.353 \t\t 1\n",
      "0 \t False \t 0.5 \t 0.361 \t\t 1\n",
      "0 \t False \t 0.5 \t 0.368 \t\t 1\n",
      "0 \t False \t 0.5 \t 0.375 \t\t 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Obs\\tDone\\tReward\\tC.Reward\\tAction\")\n",
    "verbose=True\n",
    "history = []\n",
    "rewardAcum = 0\n",
    "for i in range(20):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "    rewardAcum += reward\n",
    "    elemHist = np.array([i, reward, rewardAcum])\n",
    "    history.append(elemHist)\n",
    "    if verbose:\n",
    "        print(observation,\"\\t\", done,\"\\t\", reward, \"\\t\", \"%.3f\" % (rewardAcum/(i+1)), \"\\t\\t\", action)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9kpyAEk6aYB"
   },
   "source": [
    "# Grafique la recompensa acumulada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.5, 0.5],\n",
       "       [1. , 0.5, 1. ],\n",
       "       [2. , 0.5, 1.5],\n",
       "       [3. , 0.5, 2. ],\n",
       "       [4. , 0.5, 2.5],\n",
       "       [5. , 0. , 2.5]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to numpy\n",
    "history = np.array(history)\n",
    "history[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 0.5, 0.5, 0.5, 0. ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so I can do this\n",
    "history[0:6, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1874d6f1f28>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.grid()\n",
    "plt.xlabel(\"time step\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.xticks(np.arange(0,len(history),1))\n",
    "plt.plot(history[:,0], history[:,2], 'ro', label='Reward acumulado')\n",
    "plt.plot(history[:,0], history[:,1], 'b.', label='Reward instantáneo')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qC0PJOkV6aYH"
   },
   "source": [
    "# Calcule de forma teórica V, la value function optima para cada estado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preguntar v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #x: actual state\n",
    "    #y's: next states\n",
    "\n",
    "    V(x) = reward + gamma (sum_y( p(y|a,x) * V(y) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZRz0uMX1LbXz"
   },
   "source": [
    "# Implemente el algoritmo de iteración de valor (Value iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/value-iteration-4.4.png\" width=\"500\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVuBLOrWL1ru"
   },
   "source": [
    "  Evaluate the optimal value function given a full description of the environment dynamics\n",
    "  \n",
    "  \n",
    "\n",
    "```\n",
    " Args:\n",
    "\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "  \n",
    "  Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "```\n",
    "\n",
    "\n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: 0 , action: 0  ->  [(1.0, 0, 0.5, False)]\n",
      "state: 0 , action: 1  ->  [(0.9, 0, 0.5, False), (0.09999999999999998, 1, 0.5, False)]\n",
      "state: 0 , action: 2  ->  [(1, 0, 0, False)]\n",
      "state: 1 , action: 0  ->  [(1.0, 1, 0.5, False)]\n",
      "state: 1 , action: 1  ->  [(1, 1, 0.5, False), (0, 0, -3.0, False)]\n",
      "state: 1 , action: 2  ->  [(1.0, 0, 0.0, False)]\n"
     ]
    }
   ],
   "source": [
    "for s in range(env.nS):\n",
    "    for a in range(env.nA):\n",
    "        print(\"state:\",s, \", action:\",a, \" -> \" ,env.P[s][a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs\tDone\tReward\tC.Reward\tAction\n",
      "0 : [0. 0.]\n",
      "0 : [0.5 0. ]\n",
      "1 : [0.5 0.5]\n",
      "1 : [0.95 0.5 ]\n",
      "2 : [0.95 0.95]\n",
      "2 : [1.355 0.95 ]\n",
      "3 : [1.355 1.355]\n",
      "3 : [1.7195 1.355 ]\n",
      "4 : [1.7195 1.7195]\n",
      "4 : [2.04755 1.7195 ]\n",
      "5 : [2.04755 2.04755]\n",
      "5 : [2.342795 2.04755 ]\n",
      "6 : [2.342795 2.342795]\n",
      "6 : [2.6085155 2.342795 ]\n",
      "7 : [2.6085155 2.6085155]\n",
      "7 : [2.84766395 2.6085155 ]\n",
      "8 : [2.84766395 2.84766395]\n",
      "8 : [3.06289756 2.84766395]\n",
      "9 : [3.06289756 3.06289756]\n",
      "9 : [3.2566078  3.06289756]\n",
      "10 : [3.2566078 3.2566078]\n",
      "10 : [3.43094702 3.2566078 ]\n",
      "11 : [3.43094702 3.43094702]\n",
      "11 : [3.58785232 3.43094702]\n",
      "12 : [3.58785232 3.58785232]\n",
      "12 : [3.72906709 3.58785232]\n",
      "13 : [3.72906709 3.72906709]\n",
      "13 : [3.85616038 3.72906709]\n",
      "14 : [3.85616038 3.85616038]\n",
      "14 : [3.97054434 3.85616038]\n",
      "15 : [3.97054434 3.97054434]\n",
      "15 : [4.07348991 3.97054434]\n",
      "16 : [4.07348991 4.07348991]\n",
      "16 : [4.16614092 4.07348991]\n",
      "[4.16614092 4.16614092]\n"
     ]
    }
   ],
   "source": [
    "# en cada estado, quiero calcular la value function optima;\n",
    "# uso ecuacion de bellman, tomando el reward instantaneo y\n",
    "# estimando futuros posibles rewards a partir de las acciones.\n",
    "\n",
    "print(\"Obs\\tDone\\tReward\\tC.Reward\\tAction\")\n",
    "verbose=True\n",
    "gamma=0.9 #discount factor\n",
    "theta=0.1 #stop evaluation when updates are less than theta\n",
    "update=10\n",
    "#V = np.zeros((env.nS, env.nA))\n",
    "V = np.zeros(env.nS)\n",
    "epoch=0\n",
    "while(update > theta):\n",
    "    # bellman eq, for each state\n",
    "    for s in range(env.nS):\n",
    "        vPrev = V[s]\n",
    "        maxReward = -float('inf')\n",
    "        for a in range(env.nA):\n",
    "            sumFutureRewards = 0\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                #sum rewards over all possible next states\n",
    "                sumFutureRewards += prob * (reward + gamma*V[next_state])\n",
    "            \n",
    "            # me quedo con el mayor reward entre todas las actions a posibles en\n",
    "            # el estado s\n",
    "            maxReward = max(maxReward, sumFutureRewards)\n",
    "        print(epoch, \":\", V)\n",
    "        # me comporto greedy al elegir el valor maximo entre los obtenidos \n",
    "        V[s] = maxReward\n",
    "        update = abs(V[s] - vPrev)\n",
    "        #print(update)\n",
    "    epoch += 1\n",
    "print(V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KGWxWGNA6aYJ"
   },
   "source": [
    "# Implemente el algoritmo de policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f9o3S2kaKQKX"
   },
   "source": [
    "Definir primero una funcion de evaluación de politica,\n",
    "\n",
    "```\n",
    "Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "        \n",
    "```\n",
    "\n",
    "Despues una funcion de optimisacion de la politica:\n",
    "\n",
    "\n",
    "```\n",
    " Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "brOjUJvE6aYV"
   },
   "source": [
    "# Utilizando los 3 algoritmos, realice los experimentos para las siguientes configuraciones del ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVKvrm0t6aYV"
   },
   "outputs": [],
   "source": [
    "exp1 = generar_ambiente(alpha=0.9, beta=0.9, r_search=3, r_wait=2)\n",
    "exp2 = generar_ambiente(alpha=0.8, beta=0.5, r_search=3, r_wait=2)\n",
    "exp3 = generar_ambiente(alpha=0.5, beta=0.5, r_search=3, r_wait=2)\n",
    "exp4 = generar_ambiente(alpha=0.9, beta=0.6, r_search=1, r_wait=0.9)\n",
    "exp5 = generar_ambiente(alpha=0.9, beta=0.6, r_search=1, r_wait=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAT_YZq56aYY"
   },
   "source": [
    "# Utilizando el grafico de recompensa, compare las estrategias óptimas generadas con los experimentos anteriores contra la estrategia al azar."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TP1 - (Sin solucion).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
