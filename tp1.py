# for python3
# -*- coding: utf-8 -*-
"""TP1 - (Sin solucion).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YvarSeQi6I0PP10NjvryH4iWBM4wtE4X

**Recycling robot example** (from Sutton, page 42)
References:
  - Gym documentation: https://gym.openai.com/
"""

from gym.envs.toy_text import discrete
import random
import matplotlib.pyplot as plt
from itertools import cycle
import numpy as np

# TODO: Describir coloquialmente el modelo de sutton
"""
Agente:
Robot con 3 acciones posibles y dos estados:

Acciones:
0: Esperar
1: Buscar
2: Recargar (solo en estado 1)

Estados:
0: High
1: Low

El robot obtiene rewards por cada accion tomada en el estado que esté,
los cuales se definen por una tabla de transiciones
(como se ve en la figura 3.2 de Sutton y Barto).

"""
# TODO: Explicar lo básico de GYM
"""
GYM es una librería que se ocupa de manejar todas las transiciones y simulaciones
de un agente en un entorno.
Permite definir y generar todo en nuestro modelo, aunque también trae por defecto
varios entornos con los que podemos comparar nuestro agente/algoritmo de resolución
de modelos.
"""
"""# Considere el modelo del robot de reciclaje descríto en Sutton Example 3.2"""

states = ["high", "low"]
actions = ["wait", "search", "recharge"]

P = {}

P[0] = {}
P[1] = {}

alpha = 0.9
beta = 0.6
r_wait = 0.5
r_search = 2.0

# definimos un ambiente discreto con las transiciones según el gráfico
def generar_ambiente(alpha=alpha, beta=beta, r_wait=r_wait, r_search=r_wait):
    P[0][0] = [(1.0, 0, r_wait, False)]
    P[0][1] = [(alpha, 0, r_search, False),
               (1-alpha, 1, r_search, False)]
    P[0][2] = [(1,0,0,False)]

    P[1][0] = [(1.0, 1, r_wait, False)]
    P[1][1] = [(beta, 1, r_search, False), 
               (1-beta, 0, -3.0, False)]
    P[1][2] = [(1.0, 0, 0.0, False)]
    env = discrete.DiscreteEnv(2, 3, P, [0.0, 1.0])
    return(env)
env = generar_ambiente()

# Implemente la estrategia random para veinte episodios.
def randomExperiment(env, verbose=True):
    time_steps=20
    history = np.empty((time_steps, 3))
    rewardAcum = 0
    state=env.s # < starting state
    print("Starting at state: ", state)
    print("State\tReward\tC.Reward\tAction")
    for i in range(time_steps):
        # when on state 0 (high), action 2 (recharge) is not an option:
        actions = [0,1,2][:2+state]
        # choose a random action
        action = np.random.choice(actions)
        # take that action
        state, reward, done, info = env.step(action)
        rewardAcum += reward
        # data to return
        history[i] = np.array([i+1, reward, rewardAcum])
        if verbose:
            print(state,"\t", reward, "\t", "%.4f" % rewardAcum, "\t", action)
        env.close()
    return history

# Definir una acción aleatoria y ver que reward produce
env.reset()
env = generar_ambiente(alpha=0.9, beta=0.6, r_wait=0.5, r_search=1)
data = randomExperiment(env, verbose=True)

# Grafique la recompensa acumulada
def plotRewardFromData(data):
    plt.grid()
    plt.title("Rewards over time for random action selection")
    plt.xlabel("time step")
    plt.ylabel("reward")
    plt.xticks(np.arange(0,len(data),1))
    plt.plot(data[:,0], data[:,2], 'ro-', label='Reward acumulado', linewidth=0.4)
    plt.plot(data[:,0], data[:,1], 'b.-', label='Reward instantaneo', linewidth=0.2)

    plt.legend()
    plt.show()
plotRewardFromData(data)

# Calcule de forma teórica V, la value function optima para cada estado

# Implemente el algoritmo de iteración de valor (Value iteration)
"""
Evaluate the optimal value function given a full description of the environment dynamics
 Args:

        env: OpenAI env. env.P represents the transition probabilities of the environment.
            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).
            env.nS is a number of states in the environment. 
            env.nA is a number of actions in the environment.
        theta: We stop evaluation once our value function change is less than theta for all states.
        discount_factor: Gamma discount factor.
  
  Returns:
        Vector of length env.nS representing the value function.
"""
# En cada estado, quiero calcular la value function optima;
# uso ecuacion de bellman, tomando el reward instantaneo y
# estimando futuros posibles rewards a partir de las acciones.
def valueIteration(env, theta=0.1, gamma=0.9, verbose=False):
    if verbose:
        print("epoch\tvalue V")
    update=float('inf')
    V = np.zeros(env.nS)
    epoch=0
    while(update > theta):
        # bellman eq; for each state:
        for s in range(env.nS):
            vPrev = V[s]
            maxReward = -float('inf')
            for a in range(env.nA):
                sumFutureRewards = 0
                for prob, next_state, reward, done in env.P[s][a]:
                    # sum rewards over all possible next states
                    # (an action may result in many outcomes)
                    sumFutureRewards += prob * (reward + gamma*V[next_state])
                # selecting max reward from all possible actions 'a' at that state 's'
                if(maxReward < sumFutureRewards):
                    maxReward = sumFutureRewards
            # updating value function
            V[s] = maxReward
            #we stop when update < theta
            update = abs(V[s] - vPrev)
        epoch += 1
        if verbose:
            print(epoch, ":", V)
    return V
# Implemente el algoritmo de policy iteration
#
# Definir primero una funcion de evaluación de politica,
"""
Evaluate a policy given an environment and a full description of the environment's dynamics.
    
    Args:
        policy: [S, A] shaped matrix representing the policy.
        env: OpenAI env. env.P represents the transition probabilities of the environment.
            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).
            env.nS is a number of states in the environment. 
            env.nA is a number of actions in the environment.
        theta: We stop evaluation once our value function change is less than theta for all states.
        discount_factor: Gamma discount factor.
    
    Returns:
        Vector of length env.nS representing the value function.
        
"""
# Policy Evaluation 
# en cada estado, quiero calcular la value function PARA UNA POLICY PI;
# uso ecuacion de bellman, tomando el reward instantaneo y
# estimando futuros posibles rewards a partir de las acciones, que son elegidas
# siguiendo la policy pi.
def policyEvaluation(env, policy, theta=0.1, gamma=0.9, verbose=False):
    if verbose:
        print("epoch\tvalue V^pi_i")
    Vpi = np.zeros(env.nS)
    update=float('inf')
    epoch=0
    while(update > theta):
        # bellman eq; for each state:
        for s in range(env.nS):
            vPrev = Vpi[s]
            piReward = -float('inf')
            piReward = 0
            # the policy is deterministic, so it always choose the same action
            a = np.random.choice([0,1,2], p=policy[s])
            for prob, next_state, reward, done in env.P[s][a]:
                # sums possible rewards ONLY OVER policy's actions
                piReward += prob * (reward + gamma*Vpi[next_state])
            # this action at this state costs/reward us 'piReward'
            Vpi[s] = piReward
            update = abs(Vpi[s] - vPrev)
        epoch += 1
        if verbose:
            print(epoch, ":", Vpi)
    return Vpi

# Despues una funcion de optimisacion de la politica:
"""
 Policy Improvement Algorithm. Iteratively evaluates and improves a policy
    until an optimal policy is found.
    
    Args:
        env: The OpenAI envrionment.
        policy_eval_fn: Policy Evaluation function that takes 3 arguments:
            policy, env, discount_factor.
        discount_factor: gamma discount factor.
        
    Returns:
        A tuple (policy, V). 
        policy is the optimal policy, a matrix of shape [S, A] where each state s
        contains a valid probability distribution over actions.
        V is the value function for the optimal policy.
"""
def policyImprovement(env, policy, gamma=0.9):
    policyStable = False
    theta=0.1
    V = policyEvaluation(env, policy, theta, gamma, verbose=False)
    update=float('inf')
    while(not policyStable):
        policyStable = True
        for s in range(env.nS):
            vPrev = V
            oldA = np.array(policy[s])
            maxReward = -float('inf')
            maxAction = 0;
            for a in range(env.nA):
                sumFutureRewards = 0
                for prob, next_state, reward, done in env.P[s][a]:
                    # we weight our future reward estimation with our
                    # estimated value function
                    sumFutureRewards += prob * (reward + gamma*V[next_state])
                # me quedo con el mayor reward entre todas las actions 'a' posibles
                # en el estado s
                if(maxReward < sumFutureRewards):
                    maxReward = sumFutureRewards
                    maxAction = a
            oneHot = np.zeros(3)
            oneHot[maxAction] = 1
            policy[s] = oneHot
            if not np.array_equal(oldA, policy[s]):
                policyStable = False
        if policyStable:
            return (policy, V)
        else:
            V = policyEvaluation(env, policy, theta, gamma, verbose=False)
        update = np.mean(abs(V[s] - vPrev))

# Random policy for a given env
def testPolicyImprovementWithRandomPolicy(env, gamma=0.9):
    # generating a discrete random policy
    policy=np.zeros((env.nS, env.nA))#[S,A]
    policy[0][np.random.choice([0,1])] = 1
    policy[1][np.random.choice([0,1,2])] = 1
    print("Using Policy Improvement")
    print("Random policy to improve:")
    print(policy)
    return policyImprovement(env, policy, gamma)

# Utilizando los 3 algoritmos, realice los experimentos para las siguientes configuraciones del ambiente.
# Obs: Tuve problemas al generar varios ambientes de forma consecutiva, 
#      por lo que intercale la generacion del ambiente con su correspondiente
#      experimento..
#Discount factor for next experiments
gamma=0.9
################
# Experimento #1
# Policy optima: Buscar siempre
print("\nExperiment #1:")
exp1 = generar_ambiente(alpha=0.9, beta=0.9, r_wait=2, r_search=3)
print("Value from Value Iteration:")
print(valueIteration(env=exp1, gamma=gamma, verbose=False), "\n")
policy1, V1 = testPolicyImprovementWithRandomPolicy(env=exp1, gamma=gamma)
print("Optimal policy: \n", policy1, "\nValue from optimal policy: \n", V1)
################
# Experimento #2
# Policy optima: Buscar si con bateria alta; recargar si bateria baja.
print("\nExperiment #2:")
exp2 = generar_ambiente(alpha=0.8, beta=0.5, r_wait=2, r_search=3)
print("Value from Value Iteration:")
print(valueIteration(env=exp2, gamma=gamma, verbose=False), "\n")
policy2, V2 = testPolicyImprovementWithRandomPolicy(env=exp2, gamma=gamma)
print("Optimal policy: \n", policy2, "\nValue from optimal policy: \n", V2)
################
# Experimento #3
# Policy optima: Buscar si con bateria alta; esperar si bateria baja.
print("\nExperiment #3:")
exp3 = generar_ambiente(alpha=0.5, beta=0.5, r_wait=2, r_search=3)
print("Value from Value Iteration:")
print(valueIteration(env=exp3, gamma=gamma, verbose=False), "\n")
policy3, V3 = testPolicyImprovementWithRandomPolicy(env=exp3, gamma=gamma)
print("Optimal policy: \n", policy3, "\nValue from optimal policy: \n", V3)
################
# Experimento #4
# Policy optima (misma que Exp.#3): Buscar si con bateria alta; esperar si bateria baja.
print("\nExperiment #4:")
exp4 = generar_ambiente(alpha=0.9, beta=0.6, r_wait=0.9, r_search=1)
print("Value from Value Iteration:")
print(valueIteration(env=exp4, gamma=gamma, verbose=False), "\n")
policy4, V4 = testPolicyImprovementWithRandomPolicy(env=exp4, gamma=gamma)
print("Optimal policy: \n", policy4, "\nValue from optimal policy: \n", V4)
################
# Experimento #5
# Policy optima (misma que Exp#2): Buscar si con bateria alta; recargar si bateria baja.
print("\nExperiment #5:")
exp5 = generar_ambiente(alpha=0.9, beta=0.6, r_wait=0.5, r_search=1)
print("Value from Value Iteration:")
print(valueIteration(env=exp5, gamma=gamma, verbose=False), "\n")
policy5, V5 = testPolicyImprovementWithRandomPolicy(env=exp5, gamma=gamma)
print("Optimal policy: \n", policy5, "\nValue from optimal policy: \n", V5)
################
# Utilizando el grafico de recompensa, compare las estrategias óptimas generadas
# con los experimentos anteriores contra la estrategia al azar."""
def getExperimentData(env, time_steps=20, *policy):
    verbose=False    
    history = np.empty((time_steps, 3))
    rewardAcum = 0
    start_state=env.s # < starting state
    state=start_state
    for i in range(time_steps):
        # when on state 0, action 2 is not an option:
        actions = [0,1,2][:2+state]
        if not policy:
            # If no policy is given, we choose randomly
            action = np.random.choice(actions)
        else:
            # we follow the policy
            action = np.argmax(policy[0][state])
        # we take that action
        state, reward, done, info = env.step(action)
        rewardAcum += reward
        # data to return
        history[i] = np.array([i+1, reward, rewardAcum])
        if verbose:
            if i==0:
                print("Starting at state: ", start_state)
                print("State\tReward\tC.Reward\tAction")
            print(state,"\t", reward, "\t", "%.4f" % rewardAcum, "\t", action)
        env.close()
    return history

def plotRewardFromData(experiments, labels, title=""):
    plt.grid()
    plt.title("Rewards over time. "+title)
    plt.xlabel("time step")
    plt.ylabel("reward")
    plt.xticks(np.arange(0, len(experiments[0])+1, 1))
    #colors
    colors=["b","r","g","m","k"]
    allAcumPlots=[]
    for data,label,c in zip(experiments, labels, cycle(colors)):
        # data from some experiment #i
        #c=np.random.choice(colors)
        acumPlot, = plt.plot(data[:,0], data[:,2], c+'o-', label=label, linewidth=0.4)
        allAcumPlots.append(acumPlot) 
        plt.plot(data[:,0], data[:,1], c+'.-', label=None, linewidth=0.2)
        
    shapesLegend = plt.legend(title="Reward", 
                              labels=["acumulado", "instantaneo"],
                              loc='upper left')
    #colorsLegend = plt.legend(handles=acum, labels=["1","2"], loc='upper left', bbox_to_anchor=(0, 0.8))
    #plt.legend([a for a in allAcumPlots], title="Policy", labels=labels,
    #           loc='upper left', bbox_to_anchor=(0, 0.75))
    plt.legend(loc='upper left', bbox_to_anchor=(0, 0.75))
    plt.gca().add_artist(shapesLegend)
    plt.show()
################
# Generating data for experiment #1
exp1.reset()
# random policy
data1RandPol = getExperimentData(exp1, 20)
exp1.reset()
# optimal policy for exp1 environment
data1 = getExperimentData(exp1, 20, policy1)
# Plot random vs optimal policy
plotRewardFromData([data1RandPol, data1], ["random", "optima"], "Experiment #1")
################
# Generating data for experiment #2
exp2.reset()
data2RandPol = getExperimentData(exp2, 20)
exp2.reset()
data2 = getExperimentData(exp2, 20, policy2)
plotRewardFromData([data2RandPol, data2], ["random", "optima"], "Experiment #2")
################
# Generating data for experiment #3
exp3.reset()
data3RandPol = getExperimentData(exp3, 20)
exp3.reset()
data3 = getExperimentData(exp3, 20, policy3)
plotRewardFromData([data3RandPol, data3], ["random", "optima"], "Experiment #3")
################
# Generating data for experiment #3
exp4.reset()
data4RandPol = getExperimentData(exp4, 20)
exp4.reset()
data4 = getExperimentData(exp4, 20, policy4)
plotRewardFromData([data4RandPol, data4], ["random", "optima"], "Experiment #4")
################
# Generating data for experiment #5
exp5.reset()
data5RandPol = getExperimentData(exp5, 20)
exp5.reset()
data5 = getExperimentData(exp5, 20, policy5)
plotRewardFromData([data5RandPol, data5], ["random", "optima"], "Experiment #5")
################
# Plot of ALL optimal policies together
plotRewardFromData([data1, data2, data3, data4, data5],
                   ["opt.exp1", "opt.exp2", "opt.exp3", "opt.exp4", "opt.exp5"],
                  "All optimal policies (different envs).")
# Plot of ALL random policies together
plotRewardFromData([data1RandPol, data2RandPol, data3RandPol, data4RandPol, data5RandPol],
                   ["r.exp1"  , "r.exp2"  , "r.exp3"  , "r.exp4"  , "r.exp5"],
                  "All random policies (different envs).")
# All experiments together
plotRewardFromData([data1RandPol, data2RandPol, data3RandPol, data4RandPol, data5RandPol,
                    data1, data2, data3, data4, data5],
                   ["r.exp1"  , "r.exp2"  , "r.exp3"  , "r.exp4"  , "r.exp5",
                    "opt.exp1", "opt.exp2", "opt.exp3", "opt.exp4", "opt.exp5"],
                  "All experiments together.")
### END