{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T16:16:39.044240Z",
     "start_time": "2019-06-17T16:16:38.993264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T16:16:39.087512Z",
     "start_time": "2019-06-17T16:16:39.049544Z"
    }
   },
   "outputs": [],
   "source": [
    "#import autoreload\n",
    "#?autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T16:16:39.128630Z",
     "start_time": "2019-06-17T16:16:39.089684Z"
    }
   },
   "outputs": [],
   "source": [
    "# qlearningAgents.py\n",
    "# ------------------\n",
    "# Licensing Information:  You are free to use or extend these projects for\n",
    "# educational purposes provided that (1) you do not distribute or publish\n",
    "# solutions, (2) you retain this notice, and (3) you provide clear\n",
    "# attribution to UC Berkeley, including a link to http://ai.berkeley.edu.\n",
    "# \n",
    "# Attribution Information: The Pacman AI projects were developed at UC Berkeley.\n",
    "# The core projects and autograders were primarily created by John DeNero\n",
    "# (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n",
    "# Student side autograding was added by Brad Miller, Nick Hay, and\n",
    "# Pieter Abbeel (pabbeel@cs.berkeley.edu).\n",
    "\n",
    "\n",
    "from game import *\n",
    "from learningAgents import ReinforcementAgent\n",
    "from featureExtractors import *\n",
    "\n",
    "import random,util,math\n",
    "import numpy as np\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T16:16:39.167783Z",
     "start_time": "2019-06-17T16:16:39.132020Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_argmax(v):\n",
    "    \"\"\"Like np.argmax(), but if there are several \"best\" actions,\n",
    "       chooses and returns one randomly\"\"\"\n",
    "    arguments = np.argwhere(v == np.amax(v)).ravel()\n",
    "    return np.random.choice(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Q Learning\n",
    "\n",
    "Q(s,a) is a dictionary with each state-action value it visits  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T16:16:39.207286Z",
     "start_time": "2019-06-17T16:16:39.169069Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "class QLearningAgent(ReinforcementAgent):\n",
    "    # Parent class in learningAgents.py\n",
    "    \"\"\" Q-Learning Agent\n",
    "\n",
    "      Functions you should fill in:\n",
    "        - computeValueFromQValues\n",
    "        - computeActionFromQValues\n",
    "        - getQValue\n",
    "        - getAction\n",
    "        - update\n",
    "\n",
    "      Instance variables you have access to\n",
    "        - self.epsilon (exploration prob)\n",
    "        - self.alpha (learning rate)\n",
    "        - self.discount (discount rate)\n",
    "\n",
    "      Functions you should use\n",
    "        - self.getLegalActions(state)\n",
    "          which returns legal actions for a state\n",
    "    \"\"\"\n",
    "    def __init__(self, **args):\n",
    "        \"You can initialize Q-values here...\"\n",
    "        ReinforcementAgent.__init__(self, **args)\n",
    "        \n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        #self.Q = {}\n",
    "        self.Q = Counter()\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        \"\"\"\n",
    "          Returns Q(state,action)\n",
    "          Should return 0.0 if we have never seen a state\n",
    "          or the Q node value otherwise\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        return self.Q[(state, action)]\n",
    "\n",
    "\n",
    "    def computeValueFromQValues(self, state):\n",
    "        \"\"\"\n",
    "          Returns max_action Q(state,action)\n",
    "          where the max is over legal actions.  Note that if\n",
    "          there are no legal actions, which is the case at the\n",
    "          terminal state, you should return a value of 0.0.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # max_a(Q[state, all actions])\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            value=0.0\n",
    "        else:\n",
    "            # TODO: Find a better way\n",
    "            value=max([self.getQValue(state, a) for a in legalActions])\n",
    "        return value\n",
    "\n",
    "    def computeActionFromQValues(self, state):\n",
    "        \"\"\"\n",
    "          Compute the best action to take in a state.  Note that if there\n",
    "          are no legal actions, which is the case at the terminal state,\n",
    "          you should return None.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            action=None\n",
    "        else:\n",
    "            # TODO: Find a better way\n",
    "            action=legalActions[random_argmax([self.getQValue(state, a) for a in legalActions])]\n",
    "        return action\n",
    "\n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "          Compute the action to take in the current state.  With\n",
    "          probability self.epsilon, we should take a random action and\n",
    "          take the best policy action otherwise.  Note that if there are\n",
    "          no legal actions, which is the case at the terminal state, you\n",
    "          should choose None as the action.\n",
    "\n",
    "          HINT: You might want to use util.flipCoin(prob)\n",
    "          HINT: To pick randomly from a list, use random.choice(list)\n",
    "        \"\"\"\n",
    "        # Pick Action\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        action = None\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # epsilon decay\n",
    "        epsmin = 0.01\n",
    "        eps_decay = 0.9999\n",
    "        self.epsilon = max(self.epsilon*eps_decay, epsmin)\n",
    "        if util.flipCoin(self.epsilon):\n",
    "            # Act randomly\n",
    "            action = random.choice(legalActions)\n",
    "        else:\n",
    "            # Act greedly\n",
    "            action = self.computeActionFromQValues(state)\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    def update(self, state, action, nextState, reward):\n",
    "        \"\"\"\n",
    "          The parent class calls this to observe a\n",
    "          state = action => nextState and reward transition.\n",
    "          You should do your Q-Value update here\n",
    "\n",
    "          NOTE: You should never call this function,\n",
    "          it will be called on your behalf\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        iteration = self.episodesSoFar\n",
    "        self.alpha = 1/np.power((iteration+1), 1) # alpha decay\n",
    "        alpha = self.alpha\n",
    "        gamma = self.discount\n",
    "        # -----------------------------v revisar si calculo maximo Q\n",
    "        estimation = reward + gamma*self.computeValueFromQValues(nextState)\n",
    "        self.Q[(state, action)] += alpha*(estimation - self.Q[(state, action)])\n",
    "        #print(\"Q size:\"+str(len(self.Q)), end=\"\\r\")\n",
    "\n",
    "    def getPolicy(self, state):\n",
    "        return self.computeActionFromQValues(state)\n",
    "\n",
    "    def getValue(self, state):\n",
    "        return self.computeValueFromQValues(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T16:16:39.246415Z",
     "start_time": "2019-06-17T16:16:39.208676Z"
    },
    "cell_style": "center",
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class PacmanQAgent(QLearningAgent):\n",
    "    \"Exactly the same as QLearningAgent, but with different default parameters\"\n",
    "\n",
    "    def __init__(self, epsilon=0.05,gamma=0.8,alpha=0.2, numTraining=0, **args):\n",
    "        \"\"\"\n",
    "        These default parameters can be changed from the pacman.py command line.\n",
    "        For example, to change the exploration rate, try:\n",
    "            python pacman.py -p PacmanQLearningAgent -a epsilon=0.1\n",
    "\n",
    "        alpha    - learning rate/step size\n",
    "        epsilon  - exploration rate\n",
    "        gamma    - discount factor\n",
    "        numTraining - number of training episodes, i.e. no learning after these many episodes\n",
    "        \"\"\"\n",
    "        args['epsilon'] = epsilon\n",
    "        args['gamma'] = gamma\n",
    "        args['alpha'] = alpha\n",
    "        args['numTraining'] = numTraining\n",
    "        self.index = 0  # This is always Pacman\n",
    "        QLearningAgent.__init__(self, **args)\n",
    "\n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "        Simply calls the getAction method of QLearningAgent and then\n",
    "        informs parent of action for Pacman.  Do not change or remove this\n",
    "        method.\n",
    "        \"\"\"\n",
    "        action = QLearningAgent.getAction(self,state)\n",
    "        self.doAction(state,action)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Aproximation for Q learning\n",
    "\n",
    "Sutton and Barto Ch.9.5 - p.232 *\"Feature construction for linear methods\"*\n",
    "\n",
    "We want to generalize Q values better for different state-action pairs.\n",
    "\n",
    "Intuition:\n",
    "\n",
    "* If a ghost is close to pacman at one state and dies, we want to generalize \"danger\" to any other position where a ghost is close.\n",
    "\n",
    "\n",
    "Observations:\n",
    "\n",
    "* Linear approximators **can't** find relationships between features, so we need to combine them ourselves (if we want that)\n",
    " \n",
    " eg:\n",
    " feature \"dist_x\" represents horizontal distance to ghost,\n",
    " feature \"dist_y\" represents vertical distance to ghost,\n",
    " \n",
    " A linear approximator cannot learn if a ghost is close on a plane, because it cannot make operations inbetween features to get a combined value.\n",
    " \n",
    " To solve this, we can add a third feature that combines the other two:\n",
    " \n",
    " *feature\\[ \"dist_xy\" \\] = dist_x $*$ dist_x*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n $ : number of features\n",
    "\n",
    "$Q(s,a) = \\sum\\limits_{i=1}^n f_i(s,a) * w_i$\n",
    "\n",
    "**Prediction error:**\n",
    "\n",
    "$advantage = (R + \\gamma \\max\\limits_{a} Q(S', a)) - Q(S,A)$\n",
    "\n",
    "**Update:**\n",
    "\n",
    "$w_i \\leftarrow w_i + \\alpha \\cdot advantage \\cdot f_i(S,A)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T16:16:39.287164Z",
     "start_time": "2019-06-17T16:16:39.248209Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ApproximateQAgent(PacmanQAgent):\n",
    "    \"\"\"\n",
    "       ApproximateQLearningAgent\n",
    "\n",
    "       You should only have to overwrite getQValue\n",
    "       and update.  All other QLearningAgent functions\n",
    "       should work as is.\n",
    "    \"\"\"\n",
    "    def __init__(self, extractor='IdentityExtractor', **args):\n",
    "        #extractor = 'CoordinateExtractor'\n",
    "        self.featExtractor = util.lookup(extractor, globals())()\n",
    "        PacmanQAgent.__init__(self, **args)\n",
    "        self.weights = util.Counter()\n",
    "\n",
    "    def getWeights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        \"\"\"\n",
    "          Should return Q(state,action) = w * featureVector\n",
    "          where * is the dotProduct operator\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        featureDict = self.featExtractor.getFeatures(state, action)\n",
    "#         for feat in featureDict.keys():\n",
    "#             self.weights[feat]*featureDict[feat]\n",
    "        #print(\"aprox Q value: \", np.dot(self.weights, featureDict))\n",
    "        return np.dot(self.weights, featureDict)\n",
    "    \n",
    "    def getMaxQValue(self, state):\n",
    "        \"\"\"\n",
    "          Returns max_action Q(state,action)\n",
    "          where the max is over legal actions.  Note that if\n",
    "          there are no legal actions, which is the case at the\n",
    "          terminal state, you should return a value of 0.0.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # max_a(Q[state, all actions])\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            value=0.0\n",
    "        else:\n",
    "            # TODO: Find a better way\n",
    "            value=max([self.getQValue(state, a) for a in legalActions])\n",
    "        return value\n",
    "    \n",
    "    def update(self, state, action, nextState, reward):\n",
    "        \"\"\"\n",
    "           Should update your weights based on transition\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        iteration = self.episodesSoFar\n",
    "        self.alpha = 1/np.power((iteration+1), 1) # alpha decay\n",
    "        alpha = self.alpha\n",
    "        gamma = self.discount\n",
    "        #state = str(state)\n",
    "        featureDict = self.featExtractor.getFeatures(state, action)\n",
    "        #for key,feat in \n",
    "        \n",
    "        pastVal = self.getQValue(state, action)\n",
    "        advantage = reward + gamma*self.getMaxQValue(nextState) - pastVal\n",
    "        for feature in featureDict.keys():\n",
    "            #print(\"state: \", state, \" action: \", action)\n",
    "            self.weights[feature] += alpha * advantage * featureDict[feature]\n",
    "\n",
    "    def final(self, state):\n",
    "        \"Called at the end of each game.\"\n",
    "        # call the super-class final method\n",
    "        PacmanQAgent.final(self, state)\n",
    "\n",
    "        # did we finish training?\n",
    "        if self.episodesSoFar == self.numTraining:\n",
    "            # you might want to print your weights here for debugging\n",
    "            \"*** YOUR CODE HERE ***\"\n",
    "            \n",
    "#             print(\"Weights:\")\n",
    "#             pprint.pprint(self.weights)\n",
    "#             print(\"Features:\")\n",
    "#             for k in self.weights.keys():\n",
    "#                 state, action = k\n",
    "#                 pprint.pprint(self.featExtractor.getFeatures(state, action))\n",
    "\n",
    "#             print(len(self.weights))\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "\n",
    "* Linear model with experience replay 1:18 en lecture 5RL de Hado\n",
    "* Linear model with LSTD for solving instantly best parameter for that history\n",
    "* Non-linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/lstd-pseudocode.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T11:29:59.053449Z",
     "start_time": "2019-06-19T11:29:59.048434Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T11:30:13.799788Z",
     "start_time": "2019-06-19T11:30:13.793054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T15:37:15.531597Z",
     "start_time": "2019-06-17T15:37:15.499548Z"
    },
    "code_folding": []
   },
   "source": [
    " %%javascript\n",
    " IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T15:37:16.238640Z",
     "start_time": "2019-06-17T15:37:15.533135Z"
    }
   },
   "outputs": [],
   "source": [
    "# from subprocess import call\n",
    "# command = ('ipython nbconvert --to script qlearningAgents.ipynb')\n",
    "# call(command, shell=True)\n",
    "\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T19:15:03.483366Z",
     "start_time": "2019-06-16T19:15:03.469464Z"
    }
   },
   "outputs": [],
   "source": [
    "# state:  \n",
    "# %%%%%%%\n",
    "# %  G> %\n",
    "# % %%% %\n",
    "# % %.  %\n",
    "# % %%% %\n",
    "# %.    %\n",
    "# %%%%%%%\n",
    "# Score: -8\n",
    "\n",
    "# action:  East"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
