{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* Fijate si podes imprimir el espacio de Q a medida que avanza en el training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T15:48:40.100924Z",
     "start_time": "2019-06-21T15:48:40.079019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T15:48:40.134332Z",
     "start_time": "2019-06-21T15:48:40.102294Z"
    }
   },
   "outputs": [],
   "source": [
    "#import autoreload\n",
    "#?autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T15:48:40.170523Z",
     "start_time": "2019-06-21T15:48:40.137107Z"
    }
   },
   "outputs": [],
   "source": [
    "# qlearningAgents.py\n",
    "# ------------------\n",
    "# Licensing Information:  You are free to use or extend these projects for\n",
    "# educational purposes provided that (1) you do not distribute or publish\n",
    "# solutions, (2) you retain this notice, and (3) you provide clear\n",
    "# attribution to UC Berkeley, including a link to http://ai.berkeley.edu.\n",
    "# \n",
    "# Attribution Information: The Pacman AI projects were developed at UC Berkeley.\n",
    "# The core projects and autograders were primarily created by John DeNero\n",
    "# (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n",
    "# Student side autograding was added by Brad Miller, Nick Hay, and\n",
    "# Pieter Abbeel (pabbeel@cs.berkeley.edu).\n",
    "\n",
    "# for importing notebooks (.ipynb) as regular .py\n",
    "import import_ipynb\n",
    "from game import *\n",
    "from learningAgents import ReinforcementAgent\n",
    "from featureExtractors import *\n",
    "\n",
    "import random,util,math\n",
    "import numpy as np\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T15:48:40.204719Z",
     "start_time": "2019-06-21T15:48:40.172465Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_argmax(v):\n",
    "    \"\"\"Like np.argmax(), but if there are several \"best\" actions,\n",
    "       chooses and returns one randomly\"\"\"\n",
    "    arguments = np.argwhere(v == np.amax(v)).ravel()\n",
    "    return np.random.choice(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T16:24:10.871647Z",
     "start_time": "2019-06-21T16:24:10.831014Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# From layout of chars to layout of numbers\n",
    "# Beware: This will be painful to see\n",
    "def ascii_state_to_numeric_state(ascii_state):\n",
    "    str_state = str(ascii_state)\n",
    "    score_pos = str(str_state).find(\"Score: \")\n",
    "    ascii_map = str(str_state)[:score_pos-1]\n",
    "\n",
    "    numer_map = np.ndarray(len(ascii_map)+1, dtype=np.double)\n",
    "    for i, c in enumerate(ascii_map):\n",
    "        if c==' ':\n",
    "            numer_map[i] = 1\n",
    "            continue\n",
    "        if c=='%':\n",
    "            numer_map[i] = 2\n",
    "            continue\n",
    "        if c=='.':\n",
    "            numer_map[i] = 3\n",
    "            continue\n",
    "        if c=='\\n':\n",
    "            numer_map[i] = 4\n",
    "            continue\n",
    "        if c=='G':\n",
    "            numer_map[i] = 5\n",
    "            continue\n",
    "        if c=='o':\n",
    "            numer_map[i] = 6\n",
    "            continue\n",
    "        # Pacman dirs\n",
    "        if c=='<':\n",
    "            numer_map[i] = 7\n",
    "            continue\n",
    "        if c=='>':\n",
    "            numer_map[i] = 8\n",
    "            continue\n",
    "        if c=='^':\n",
    "            numer_map[i] = 9\n",
    "            continue\n",
    "        if c=='v':\n",
    "            numer_map[i] = 10\n",
    "            continue\n",
    "    numer_map /= 15.0\n",
    "    #last array position will contain the score\n",
    "    numer_map[-1] = float(str_state[score_pos+7:])/3000\n",
    "    return numer_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Q Learning\n",
    "\n",
    "Q(s,a) is a dictionary with each state-action value it visits  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T15:48:40.275654Z",
     "start_time": "2019-06-21T15:48:40.238927Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "class QLearningAgent(ReinforcementAgent):\n",
    "    # Parent class in learningAgents.py\n",
    "    \"\"\" Q-Learning Agent\n",
    "\n",
    "      Functions you should fill in:\n",
    "        - computeValueFromQValues\n",
    "        - computeActionFromQValues\n",
    "        - getQValue\n",
    "        - getAction\n",
    "        - update\n",
    "\n",
    "      Instance variables you have access to\n",
    "        - self.epsilon (exploration prob)\n",
    "        - self.alpha (learning rate)\n",
    "        - self.discount (discount rate)\n",
    "\n",
    "      Functions you should use\n",
    "        - self.getLegalActions(state)\n",
    "          which returns legal actions for a state\n",
    "    \"\"\"\n",
    "    def __init__(self, **args):\n",
    "        \"You can initialize Q-values here...\"\n",
    "        ReinforcementAgent.__init__(self, **args)\n",
    "        \n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        #self.Q = {}\n",
    "        self.Q = Counter()\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        \"\"\"\n",
    "          Returns Q(state,action)\n",
    "          Should return 0.0 if we have never seen a state\n",
    "          or the Q node value otherwise\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        return self.Q[(state, action)]\n",
    "\n",
    "\n",
    "    def computeValueFromQValues(self, state):\n",
    "        \"\"\"\n",
    "          Returns max_action Q(state,action)\n",
    "          where the max is over legal actions.  Note that if\n",
    "          there are no legal actions, which is the case at the\n",
    "          terminal state, you should return a value of 0.0.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # max_a(Q[state, all actions])\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            value=0.0\n",
    "        else:\n",
    "            # TODO: Find a better way\n",
    "            value=max([self.getQValue(state, a) for a in legalActions])\n",
    "        return value\n",
    "\n",
    "    def computeActionFromQValues(self, state):\n",
    "        \"\"\"\n",
    "          Compute the best action to take in a state.  Note that if there\n",
    "          are no legal actions, which is the case at the terminal state,\n",
    "          you should return None.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            action=None\n",
    "        else:\n",
    "            # TODO: Find a better way\n",
    "            action=legalActions[random_argmax([self.getQValue(state, a) for a in legalActions])]\n",
    "        return action\n",
    "\n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "          Compute the action to take in the current state.  With\n",
    "          probability self.epsilon, we should take a random action and\n",
    "          take the best policy action otherwise.  Note that if there are\n",
    "          no legal actions, which is the case at the terminal state, you\n",
    "          should choose None as the action.\n",
    "\n",
    "          HINT: You might want to use util.flipCoin(prob)\n",
    "          HINT: To pick randomly from a list, use random.choice(list)\n",
    "        \"\"\"\n",
    "        # Pick Action\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        action = None\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # epsilon decay\n",
    "        epsmin = 0.01\n",
    "        eps_decay = 0.9999\n",
    "        self.epsilon = max(self.epsilon*eps_decay, epsmin)\n",
    "        if util.flipCoin(self.epsilon):\n",
    "            # Act randomly\n",
    "            action = random.choice(legalActions)\n",
    "        else:\n",
    "            # Act greedly\n",
    "            action = self.computeActionFromQValues(state)\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    def update(self, state, action, nextState, reward):\n",
    "        \"\"\"\n",
    "          The parent class calls this to observe a\n",
    "          state = action => nextState and reward transition.\n",
    "          You should do your Q-Value update here\n",
    "\n",
    "          NOTE: You should never call this function,\n",
    "          it will be called on your behalf\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        iteration = self.episodesSoFar\n",
    "        self.alpha = 1/np.power((iteration+1), 1) # alpha decay\n",
    "        alpha = self.alpha\n",
    "        gamma = self.discount\n",
    "        # -----------------------------v revisar si calculo maximo Q\n",
    "        estimation = reward + gamma*self.computeValueFromQValues(nextState)\n",
    "        self.Q[(state, action)] += alpha*(estimation - self.Q[(state, action)])\n",
    "        #print(\"Q size:\"+str(len(self.Q)), end=\"\\r\")\n",
    "\n",
    "    def getPolicy(self, state):\n",
    "        return self.computeActionFromQValues(state)\n",
    "\n",
    "    def getValue(self, state):\n",
    "        return self.computeValueFromQValues(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T15:48:40.315704Z",
     "start_time": "2019-06-21T15:48:40.277384Z"
    },
    "cell_style": "center",
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class PacmanQAgent(QLearningAgent):\n",
    "    \"Exactly the same as QLearningAgent, but with different default parameters\"\n",
    "\n",
    "    def __init__(self, epsilon=0.05,gamma=0.8,alpha=0.2, numTraining=0, **args):\n",
    "        \"\"\"\n",
    "        These default parameters can be changed from the pacman.py command line.\n",
    "        For example, to change the exploration rate, try:\n",
    "            python pacman.py -p PacmanQLearningAgent -a epsilon=0.1\n",
    "\n",
    "        alpha    - learning rate/step size\n",
    "        epsilon  - exploration rate\n",
    "        gamma    - discount factor\n",
    "        numTraining - number of training episodes, i.e. no learning after these many episodes\n",
    "        \"\"\"\n",
    "        args['epsilon'] = epsilon\n",
    "        args['gamma'] = gamma\n",
    "        args['alpha'] = alpha\n",
    "        args['numTraining'] = numTraining\n",
    "        self.index = 0  # This is always Pacman\n",
    "        QLearningAgent.__init__(self, **args)\n",
    "\n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "        Simply calls the getAction method of QLearningAgent and then\n",
    "        informs parent of action for Pacman.  Do not change or remove this\n",
    "        method.\n",
    "        \"\"\"\n",
    "        action = QLearningAgent.getAction(self,state)\n",
    "        self.doAction(state,action)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Aproximation for Q learning\n",
    "\n",
    "Sutton and Barto Ch.9.5 - p.232 *\"Feature construction for linear methods\"*\n",
    "\n",
    "We want to generalize Q values better for different state-action pairs.\n",
    "\n",
    "Intuition:\n",
    "\n",
    "* If a ghost is close to pacman at one state and dies, we want to generalize \"danger\" to any other position where a ghost is close.\n",
    "\n",
    "\n",
    "Observations:\n",
    "\n",
    "* Linear approximators **can't** find relationships between features, so we need to combine them ourselves (if we want that)\n",
    " \n",
    " eg:\n",
    " feature \"dist_x\" represents horizontal distance to ghost,\n",
    " feature \"dist_y\" represents vertical distance to ghost,\n",
    " \n",
    " A linear approximator cannot learn if a ghost is close on a plane, because it cannot make operations inbetween features to get a combined value.\n",
    " \n",
    " To solve this, we can add a third feature that combines the other two:\n",
    " \n",
    " *feature\\[ \"dist_xy\" \\] = dist_x $*$ dist_x*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n $ : number of features\n",
    "\n",
    "$Q(s,a) = \\sum\\limits_{i=1}^n f_i(s,a) * w_i$\n",
    "\n",
    "**Prediction error:**\n",
    "\n",
    "$advantage = (R + \\gamma \\max\\limits_{a} Q(S', a)) - Q(S,A)$\n",
    "\n",
    "**Update:**\n",
    "\n",
    "$w_i \\leftarrow w_i + \\alpha \\cdot advantage \\cdot f_i(S,A)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T15:48:40.351753Z",
     "start_time": "2019-06-21T15:48:40.317621Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ApproximateQAgent(PacmanQAgent):\n",
    "    \"\"\"\n",
    "       ApproximateQLearningAgent\n",
    "\n",
    "       You should only have to overwrite getQValue\n",
    "       and update.  All other QLearningAgent functions\n",
    "       should work as is.\n",
    "    \"\"\"\n",
    "    def __init__(self, extractor='IdentityExtractor', **args):\n",
    "        #extractor = 'CoordinateExtractor'\n",
    "        self.featExtractor = util.lookup(extractor, globals())()\n",
    "        PacmanQAgent.__init__(self, **args)\n",
    "        self.weights = util.Counter()\n",
    "\n",
    "    def getWeights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        \"\"\"\n",
    "          Should return Q(state,action) = w * featureVector\n",
    "          where * is the dotProduct operator\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        \n",
    "        featureDict = self.featExtractor.getFeatures(state, action)\n",
    "#         for feat in featureDict.keys():\n",
    "#             self.weights[feat]*featureDict[feat]\n",
    "        #print(\"aprox Q value: \", np.dot(self.weights, featureDict))\n",
    "        return np.dot(self.weights, featureDict)\n",
    "    \n",
    "    def getMaxQValue(self, state):\n",
    "        \"\"\"\n",
    "          Returns max_action Q(state,action)\n",
    "          where the max is over legal actions.  Note that if\n",
    "          there are no legal actions, which is the case at the\n",
    "          terminal state, you should return a value of 0.0.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # max_a(Q[state, all actions])\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            value=0.0\n",
    "        else:\n",
    "            # TODO: Find a better way\n",
    "            value=max([self.getQValue(state, a) for a in legalActions])\n",
    "        return value\n",
    "    \n",
    "    def update(self, state, action, nextState, reward):\n",
    "        \"\"\"\n",
    "           Should update your weights based on transition\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        iteration = self.episodesSoFar\n",
    "        self.alpha = 1/np.power((iteration+1), 1) # alpha decay\n",
    "        alpha = self.alpha\n",
    "        gamma = self.discount\n",
    "        #state = str(state)\n",
    "        featureDict = self.featExtractor.getFeatures(state, action)\n",
    "        #for key,feat in \n",
    "        \n",
    "        pastVal = self.getQValue(state, action)\n",
    "        advantage = reward + gamma*self.getMaxQValue(nextState) - pastVal\n",
    "        for feature in featureDict.keys():\n",
    "            #print(\"state: \", state, \" action: \", action)\n",
    "            self.weights[feature] += alpha * advantage * featureDict[feature]\n",
    "\n",
    "    def final(self, state):\n",
    "        \"Called at the end of each game.\"\n",
    "        # call the super-class final method\n",
    "        PacmanQAgent.final(self, state)\n",
    "\n",
    "        # did we finish training?\n",
    "        if self.episodesSoFar == self.numTraining:\n",
    "            # you might want to print your weights here for debugging\n",
    "            \"*** YOUR CODE HERE ***\"\n",
    "            \n",
    "#             print(\"Weights:\")\n",
    "#             pprint.pprint(self.weights)\n",
    "#             print(\"Features:\")\n",
    "#             for k in self.weights.keys():\n",
    "#                 state, action = k\n",
    "#                 pprint.pprint(self.featExtractor.getFeatures(state, action))\n",
    "\n",
    "#             print(len(self.weights))\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "\n",
    "* Linear model with experience replay 1:18 en lecture 5RL de Hado\n",
    "* Linear model with LSTD for solving instantly best parameter for that history\n",
    "* Non-linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTD (still empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/lstd-pseudocode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For number of features: 100s-1000s\n",
    "\n",
    "For millones of features it becomes unwieldy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T15:48:40.379498Z",
     "start_time": "2019-06-21T15:48:40.353375Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# make it work\n",
    "# add forgetting\n",
    "class LSTDAgent(PacmanQAgent):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodic Semi-gradient Sarsa for Estimating Q*(s,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T22:54:22.023385Z",
     "start_time": "2019-06-20T22:54:21.747231Z"
    }
   },
   "source": [
    "![](../img/episodic-semi-gradient-sarsa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO / TOREMEMBER\n",
    "* Con el aproximador lineal tenia\n",
    "\n",
    " Q(s,a) = $\\sum$ feature(s,a)*wi\n",
    "\n",
    "\n",
    "* Ac√° voy a tener una NN que tiene :\n",
    "\n",
    "\n",
    "1. de entrada 2 valores: state-action pair\n",
    "2. hidden layers no se\n",
    "3. output layer cantidad de features\n",
    "\n",
    "\n",
    "* Ej: \n",
    "\n",
    "\n",
    "1. Voy a necesitar predecir valores a partir de valores de entrada\n",
    "2. Voy a necesitar los gradientes para pesar la Advantage Function\n",
    "3. Voy a necesitar actualizar los pesos de mi red neuronal <- tal vez lo pueda definir en la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T15:48:40.410400Z",
     "start_time": "2019-06-21T15:48:40.381141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=148, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        #self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        #self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        # input: 147 chars from state and 1 from action taken\n",
    "        self.fc1 = nn.Linear(148, 100)  # \n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        #x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        #x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        #x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T16:09:15.414720Z",
     "start_time": "2019-06-21T16:09:15.366762Z"
    }
   },
   "outputs": [],
   "source": [
    "class NNQAgent(PacmanQAgent):\n",
    "    \"\"\"\n",
    "       ApproximateQLearningAgent\n",
    "\n",
    "       You should only have to overwrite getQValue\n",
    "       and update.  All other QLearningAgent functions\n",
    "       should work as is.\n",
    "    \"\"\"\n",
    "    def __init__(self, extractor='IdentityExtractor', **args):\n",
    "        #extractor = 'CoordinateExtractor'\n",
    "        #self.featExtractor = util.lookup(extractor, globals())()\n",
    "        PacmanQAgent.__init__(self, **args)\n",
    "        #self.weights = util.Counter()\n",
    "        self.net = self.initNN()\n",
    "        # to float; test with double later\n",
    "        self.net = self.net.float() \n",
    "        \n",
    "    def initNN(self):\n",
    "        net = Net()\n",
    "        # Create random Tensors for weights.\n",
    "        # Setting requires_grad=True indicates that we want to compute gradients with\n",
    "        # respect to these Tensors during the backward pass.\n",
    "#         self.w1 = torch.randn(D_in, H1, device=device, dtype=dtype, requires_grad=True)\n",
    "#         self.w2 = torch.randn(H1,   H2, device=device, dtype=dtype, requires_grad=True)\n",
    "#         self.w3 = torch.randn(H2, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "        return net\n",
    "\n",
    "    def getWeights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        \"\"\"\n",
    "          Should return Q(state,action) = w * featureVector\n",
    "          where * is the dotProduct operator\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        #featureDict = self.featExtractor.getFeatures(state, action)\n",
    "#         for feat in featureDict.keys():\n",
    "#             self.weights[feat]*featureDict[feat]\n",
    "        #print(\"aprox Q value: \", np.dot(self.weights, featureDict))\n",
    "        #return np.dot(self.weights, featureDict)       \n",
    "        numer_state = ascii_state_to_numeric_state(state)\n",
    "        actions = {'North':1./6,'South':2./6,'East':3./6,'West':4./6,'Stop':5./6}\n",
    "        numer_action = actions[action]\n",
    "        input = torch.from_numpy(np.array([numer_state, numer_action]))\n",
    "        return self.net(input)\n",
    "    \n",
    "    def computeActionFromNN(self, state):\n",
    "        \"\"\"\n",
    "          Compute the best action to take in a state.  Note that if there\n",
    "          are no legal actions, which is the case at the terminal state,\n",
    "          you should return None.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            action=None\n",
    "        else:\n",
    "            # TODO: Find a better way\n",
    "            #action=legalActions[random_argmax([self.getQValue(state, a) for a in legalActions])]\n",
    "            numer_state = ascii_state_to_numeric_state(state)\n",
    "            actions = {'North':[1./6],\n",
    "                       'South':[2./6],\n",
    "                       'East' :[3./6],\n",
    "                       'West' :[4./6],\n",
    "                       'Stop' :[5./6]}\n",
    "            #print(numer_state)\n",
    "            #print(actions['East'])\n",
    "            input_data = np.concatenate((numer_state, actions['East']))\n",
    "            something = torch.from_numpy(input_data.astype(dtype=np.double))\n",
    "            \n",
    "            #all_q_s_values = [self.net(torch.from_numpy(np.concatenate((numer_state, actions[a])))) for a in legalActions]\n",
    "            all_q_s_values = [self.net(torch.from_numpy(np.concatenate((numer_state, actions[a])).astype(dtype=np.double)).double()) for a in legalActions]\n",
    "            best_action = random_argmax(all_q_s_values)\n",
    "            action=legalActions[best_action]\n",
    "        return action\n",
    "\n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "          Compute the action to take in the current state.  With\n",
    "          probability self.epsilon, we should take a random action and\n",
    "          take the best policy action otherwise.  Note that if there are\n",
    "          no legal actions, which is the case at the terminal state, you\n",
    "          should choose None as the action.\n",
    "\n",
    "          HINT: You might want to use util.flipCoin(prob)\n",
    "          HINT: To pick randomly from a list, use random.choice(list)\n",
    "        \"\"\"\n",
    "        # Pick Action\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        action = None\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # epsilon decay\n",
    "        epsmin = 0.01\n",
    "        eps_decay = 0.9999\n",
    "        self.epsilon = max(self.epsilon*eps_decay, epsmin)\n",
    "        if util.flipCoin(self.epsilon):\n",
    "            # Act randomly\n",
    "            action = random.choice(legalActions)\n",
    "        else:\n",
    "            # Act greedly\n",
    "            action = self.computeActionFromNN(state)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def getMaxQValue(self, state):\n",
    "        \"\"\"\n",
    "          Returns max_action Q(state,action)\n",
    "          where the max is over legal actions.  Note that if\n",
    "          there are no legal actions, which is the case at the\n",
    "          terminal state, you should return a value of 0.0.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # max_a(Q[state, all actions])\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            value=0.0\n",
    "        else:\n",
    "            # TODO: Find a better way\n",
    "            value=max([self.getQValue(state, a) for a in legalActions])\n",
    "        return value\n",
    "    \n",
    "    def update(self, state, action, nextState, reward):\n",
    "        \"\"\"\n",
    "           Should update your weights based on transition\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        iteration = self.episodesSoFar\n",
    "        self.alpha = 1/np.power((iteration+1), 1) # alpha decay\n",
    "        alpha = self.alpha\n",
    "        gamma = self.discount\n",
    "        #state = str(state)\n",
    "        #featureDict = self.featExtractor.getFeatures(state, action)\n",
    "        #for key,feat in \n",
    "        \n",
    "        #pastVal = self.getQValue(state, action)\n",
    "        pastVal = self.getQValue(state, action)\n",
    "        advantage = reward + gamma*self.getMaxQValue(nextState) - pastVal\n",
    "        #for feature in featureDict.keys():\n",
    "        for name, param in net.named_parameters():\n",
    "            #print(\"state: \", state, \" action: \", action)\n",
    "            #self.weights[feature] += alpha * advantage * featureDict[feature]\n",
    "            param += alpha * advantage * param\n",
    "\n",
    "    def final(self, state):\n",
    "        \"Called at the end of each game.\"\n",
    "        # call the super-class final method\n",
    "        PacmanQAgent.final(self, state)\n",
    "\n",
    "        # did we finish training?\n",
    "        if self.episodesSoFar == self.numTraining:\n",
    "            # you might want to print your weights here for debugging\n",
    "            \"*** YOUR CODE HERE ***\"\n",
    "            \n",
    "#             print(\"Weights:\")\n",
    "#             pprint.pprint(self.weights)\n",
    "#             print(\"Features:\")\n",
    "#             for k in self.weights.keys():\n",
    "#                 state, action = k\n",
    "#                 pprint.pprint(self.featExtractor.getFeatures(state, action))\n",
    "\n",
    "#             print(len(self.weights))\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T15:37:16.238640Z",
     "start_time": "2019-06-17T15:37:15.533135Z"
    }
   },
   "outputs": [],
   "source": [
    "# from subprocess import call\n",
    "# command = ('ipython nbconvert --to script qlearningAgents.ipynb')\n",
    "# call(command, shell=True)\n",
    "\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T15:37:25.404889Z",
     "start_time": "2019-06-21T15:37:25.374012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.array(1),2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T13:32:34.269845Z",
     "start_time": "2019-06-21T13:32:34.203680Z"
    }
   },
   "source": [
    "Small Classic Layout (7x20):\n",
    "\n",
    "    %%%%%%%%%%%%%%%%%%%%\n",
    "    %......%G  G%......%\n",
    "    %.%%...%%  %%...%%.%\n",
    "    %.%o.%........%.o%.%\n",
    "    %.%%.%.%%%%%%.%.%%.%\n",
    "    %........P.........%\n",
    "    %%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "I need it as input of my NN, flatting it out (1x140):\n",
    "\n",
    "    %%%%%%%%%%%%%%%%%%%%%......%G  G%......%%.%%...%%  %%...%%.%%.%o.%........%.o%.%%.%%.%.%%%%%%.%.%%.%%........P.........%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "Still need numbers/one-hot vectors for each symbol? or will it work without?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T19:15:03.483366Z",
     "start_time": "2019-06-16T19:15:03.469464Z"
    }
   },
   "outputs": [],
   "source": [
    "# state:  \n",
    "# %%%%%%%\n",
    "# %  G> %\n",
    "# % %%% %\n",
    "# % %.  %\n",
    "# % %%% %\n",
    "# %.    %\n",
    "# %%%%%%%\n",
    "# Score: -8\n",
    "\n",
    "# action:  East"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
