{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T19:15:02.595613Z",
     "start_time": "2019-06-16T19:15:02.578386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T19:15:02.612780Z",
     "start_time": "2019-06-16T19:15:02.597824Z"
    }
   },
   "outputs": [],
   "source": [
    "#import autoreload\n",
    "#?autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T19:15:02.627217Z",
     "start_time": "2019-06-16T19:15:02.614206Z"
    }
   },
   "outputs": [],
   "source": [
    "# qlearningAgents.py\n",
    "# ------------------\n",
    "# Licensing Information:  You are free to use or extend these projects for\n",
    "# educational purposes provided that (1) you do not distribute or publish\n",
    "# solutions, (2) you retain this notice, and (3) you provide clear\n",
    "# attribution to UC Berkeley, including a link to http://ai.berkeley.edu.\n",
    "# \n",
    "# Attribution Information: The Pacman AI projects were developed at UC Berkeley.\n",
    "# The core projects and autograders were primarily created by John DeNero\n",
    "# (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n",
    "# Student side autograding was added by Brad Miller, Nick Hay, and\n",
    "# Pieter Abbeel (pabbeel@cs.berkeley.edu).\n",
    "\n",
    "\n",
    "from game import *\n",
    "from learningAgents import ReinforcementAgent\n",
    "from featureExtractors import *\n",
    "\n",
    "import random,util,math\n",
    "import numpy as np\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T19:15:02.651024Z",
     "start_time": "2019-06-16T19:15:02.628778Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_argmax(v):\n",
    "    \"\"\"Like np.argmax(), but if there are several \"best\" actions,\n",
    "       chooses and returns one randomly\"\"\"\n",
    "    arguments = np.argwhere(v == np.amax(v)).ravel()\n",
    "    return np.random.choice(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T19:15:02.676643Z",
     "start_time": "2019-06-16T19:15:02.653666Z"
    }
   },
   "outputs": [],
   "source": [
    "class QLearningAgent(ReinforcementAgent):\n",
    "    \"\"\"\n",
    "      Q-Learning Agent\n",
    "\n",
    "      Functions you should fill in:\n",
    "        - computeValueFromQValues\n",
    "        - computeActionFromQValues\n",
    "        - getQValue\n",
    "        - getAction\n",
    "        - update\n",
    "\n",
    "      Instance variables you have access to\n",
    "        - self.epsilon (exploration prob)\n",
    "        - self.alpha (learning rate)\n",
    "        - self.discount (discount rate)\n",
    "\n",
    "      Functions you should use\n",
    "        - self.getLegalActions(state)\n",
    "          which returns legal actions for a state\n",
    "    \"\"\"\n",
    "    def __init__(self, **args):\n",
    "        \"You can initialize Q-values here...\"\n",
    "        ReinforcementAgent.__init__(self, **args)\n",
    "        \n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        #self.Q = {}\n",
    "        self.Q = Counter()\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        \"\"\"\n",
    "          Returns Q(state,action)\n",
    "          Should return 0.0 if we have never seen a state\n",
    "          or the Q node value otherwise\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        return self.Q[(state, action)]\n",
    "\n",
    "\n",
    "    def computeValueFromQValues(self, state):\n",
    "        \"\"\"\n",
    "          Returns max_action Q(state,action)\n",
    "          where the max is over legal actions.  Note that if\n",
    "          there are no legal actions, which is the case at the\n",
    "          terminal state, you should return a value of 0.0.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # max_a(Q[state, all actions])\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            value=0.0\n",
    "        else:\n",
    "            # TODO: Find a better way\n",
    "            value=max([self.getQValue(state, a) for a in legalActions])\n",
    "        return value\n",
    "\n",
    "    def computeActionFromQValues(self, state):\n",
    "        \"\"\"\n",
    "          Compute the best action to take in a state.  Note that if there\n",
    "          are no legal actions, which is the case at the terminal state,\n",
    "          you should return None.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            action=None\n",
    "        else:\n",
    "            # TODO: Find a better way\n",
    "            action=legalActions[random_argmax([self.getQValue(state, a) for a in legalActions])]\n",
    "        return action\n",
    "\n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "          Compute the action to take in the current state.  With\n",
    "          probability self.epsilon, we should take a random action and\n",
    "          take the best policy action otherwise.  Note that if there are\n",
    "          no legal actions, which is the case at the terminal state, you\n",
    "          should choose None as the action.\n",
    "\n",
    "          HINT: You might want to use util.flipCoin(prob)\n",
    "          HINT: To pick randomly from a list, use random.choice(list)\n",
    "        \"\"\"\n",
    "        # Pick Action\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        action = None\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        if util.flipCoin(self.epsilon):\n",
    "            # Act randomly\n",
    "            action = random.choice(legalActions)\n",
    "        else:\n",
    "            # Act greedly\n",
    "            action = self.computeActionFromQValues(state)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, nextState, reward):\n",
    "        \"\"\"\n",
    "          The parent class calls this to observe a\n",
    "          state = action => nextState and reward transition.\n",
    "          You should do your Q-Value update here\n",
    "\n",
    "          NOTE: You should never call this function,\n",
    "          it will be called on your behalf\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        alpha = self.alpha\n",
    "        gamma = self.discount\n",
    "        # -----------------------------v revisar si calculo maximo Q\n",
    "        estimation = reward + gamma*self.computeValueFromQValues(nextState)\n",
    "        self.Q[(state, action)] += alpha*(estimation - self.Q[(state, action)])\n",
    "        #print(\"Q size:\"+str(len(self.Q)), end=\"\\r\")\n",
    "\n",
    "    def getPolicy(self, state):\n",
    "        return self.computeActionFromQValues(state)\n",
    "\n",
    "    def getValue(self, state):\n",
    "        return self.computeValueFromQValues(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T19:15:02.707439Z",
     "start_time": "2019-06-16T19:15:02.681980Z"
    },
    "cell_style": "center",
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class PacmanQAgent(QLearningAgent):\n",
    "    \"Exactly the same as QLearningAgent, but with different default parameters\"\n",
    "\n",
    "    def __init__(self, epsilon=0.05,gamma=0.8,alpha=0.2, numTraining=0, **args):\n",
    "        \"\"\"\n",
    "        These default parameters can be changed from the pacman.py command line.\n",
    "        For example, to change the exploration rate, try:\n",
    "            python pacman.py -p PacmanQLearningAgent -a epsilon=0.1\n",
    "\n",
    "        alpha    - learning rate/step size\n",
    "        epsilon  - exploration rate\n",
    "        gamma    - discount factor\n",
    "        numTraining - number of training episodes, i.e. no learning after these many episodes\n",
    "        \"\"\"\n",
    "        args['epsilon'] = epsilon\n",
    "        args['gamma'] = gamma\n",
    "        args['alpha'] = alpha\n",
    "        args['numTraining'] = numTraining\n",
    "        self.index = 0  # This is always Pacman\n",
    "        QLearningAgent.__init__(self, **args)\n",
    "\n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "        Simply calls the getAction method of QLearningAgent and then\n",
    "        informs parent of action for Pacman.  Do not change or remove this\n",
    "        method.\n",
    "        \"\"\"\n",
    "        action = QLearningAgent.getAction(self,state)\n",
    "        self.doAction(state,action)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T19:15:02.725527Z",
     "start_time": "2019-06-16T19:15:02.709205Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ApproximateQAgent(PacmanQAgent):\n",
    "    \"\"\"\n",
    "       ApproximateQLearningAgent\n",
    "\n",
    "       You should only have to overwrite getQValue\n",
    "       and update.  All other QLearningAgent functions\n",
    "       should work as is.\n",
    "    \"\"\"\n",
    "    def __init__(self, extractor='IdentityExtractor', **args):\n",
    "        #extractor = 'CoordinateExtractor'\n",
    "        self.featExtractor = util.lookup(extractor, globals())()\n",
    "        PacmanQAgent.__init__(self, **args)\n",
    "        self.weights = util.Counter()\n",
    "\n",
    "    def getWeights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        \"\"\"\n",
    "          Should return Q(state,action) = w * featureVector\n",
    "          where * is the dotProduct operator\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        featureDict = self.featExtractor.getFeatures(state, action)\n",
    "#         for feat in featureDict.keys():\n",
    "#             self.weights[feat]*featureDict[feat]\n",
    "        #print(\"aprox Q value: \", np.dot(self.weights, featureDict))\n",
    "        return np.dot(self.weights, featureDict)\n",
    "    \n",
    "    def getMaxQValue(self, state):\n",
    "        \"\"\"\n",
    "          Returns max_action Q(state,action)\n",
    "          where the max is over legal actions.  Note that if\n",
    "          there are no legal actions, which is the case at the\n",
    "          terminal state, you should return a value of 0.0.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        # max_a(Q[state, all actions])\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if not legalActions:\n",
    "            value=0.0\n",
    "        else:\n",
    "            # TODO: Find a better way\n",
    "            value=max([self.getQValue(state, a) for a in legalActions])\n",
    "        return value\n",
    "    \n",
    "    def update(self, state, action, nextState, reward):\n",
    "        \"\"\"\n",
    "           Should update your weights based on transition\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        alpha = self.alpha\n",
    "        #alpha = 0.05\n",
    "        gamma = self.discount\n",
    "        #state = str(state)\n",
    "        featureDict = self.featExtractor.getFeatures(state, action)\n",
    "        #for key,feat in \n",
    "        estimation = reward + gamma*self.getMaxQValue(nextState)\n",
    "        pastVal = self.getQValue(state, action)\n",
    "        for feature in featureDict.keys():\n",
    "            #print(\"state: \", state, \" action: \", action)\n",
    "            self.weights[feature] += alpha * (estimation - pastVal) * featureDict[feature]\n",
    "\n",
    "    def final(self, state):\n",
    "        \"Called at the end of each game.\"\n",
    "        # call the super-class final method\n",
    "        PacmanQAgent.final(self, state)\n",
    "\n",
    "        # did we finish training?\n",
    "        if self.episodesSoFar == self.numTraining:\n",
    "            # you might want to print your weights here for debugging\n",
    "            \"*** YOUR CODE HERE ***\"\n",
    "            \n",
    "#             print(\"Weights:\")\n",
    "#             pprint.pprint(self.weights)\n",
    "#             print(\"Features:\")\n",
    "#             for k in self.weights.keys():\n",
    "#                 state, action = k\n",
    "#                 pprint.pprint(self.featExtractor.getFeatures(state, action))\n",
    "\n",
    "#             print(len(self.weights))\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T19:15:02.740763Z",
     "start_time": "2019-06-16T19:15:02.727078Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_notebook()"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T19:15:03.467509Z",
     "start_time": "2019-06-16T19:15:02.742467Z"
    }
   },
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "command = ('ipython nbconvert --to script qlearningAgents.ipynb')\n",
    "call(command, shell=True)\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T19:15:03.483366Z",
     "start_time": "2019-06-16T19:15:03.469464Z"
    }
   },
   "outputs": [],
   "source": [
    "# state:  \n",
    "# %%%%%%%\n",
    "# %  G> %\n",
    "# % %%% %\n",
    "# % %.  %\n",
    "# % %%% %\n",
    "# %.    %\n",
    "# %%%%%%%\n",
    "# Score: -8\n",
    "\n",
    "# action:  East"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
