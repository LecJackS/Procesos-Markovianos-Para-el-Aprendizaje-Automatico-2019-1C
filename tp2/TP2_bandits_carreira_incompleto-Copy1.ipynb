{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.2 (default, Dec 29 2018, 06:19:36) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliBanditEnv(object): \n",
    "    # Clase que define el environment donde el reward es 0 o 1 dependiendo de una probabilidad p.\n",
    "\n",
    "    def __init__(self, num_arms=10, p=None):\n",
    "        self.num_arms = num_arms\n",
    "        self.actions = np.arange(num_arms)     # define set of actions\n",
    "\n",
    "        if len(p)==1:\n",
    "            self.p = np.random.beta(0.5, 0.5, size=num_arms)\n",
    "        elif len(p) == num_arms:\n",
    "            self.p = p\n",
    "        else:\n",
    "            raise Exception('Number of probabilities ({}) does not correspond to number of arms ({}).'.format(len(q), num_arms))\n",
    "        self.best_action = np.argmax(self.p)   # La mejor accion dado el enviroenmenteneas\n",
    "\n",
    "    def reward(self, action):\n",
    "        return np.random.binomial(1, p=self.p[action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    # Clase que define al agente. Cada agente cuenta con una regla de decisión y una regla de aprendizaje.\n",
    "    \n",
    "    def __init__(self, learning_rule, decision_rule, param=None):\n",
    "        self.decision_rule = decision_rule\n",
    "        self.learning_rule = learning_rule\n",
    "\n",
    "        if decision_rule == \"epsilon-greedy\":\n",
    "            self.epsilon = param[\"epsilon\"]\n",
    "        \n",
    "        if decision_rule == \"UCB\":\n",
    "            self.UCB_param = param[\"UCB_param\"]\n",
    "        \n",
    "        if decision_rule == \"gradient\":\n",
    "            self.lr = param[\"lr\"]\n",
    "\n",
    "    def environment(self, env, init_q):\n",
    "        # inicializa el environment\n",
    "        self.env = env                                  \n",
    "        self.k = env.num_arms                           \n",
    "        self.actions = np.arange(self.k)                \n",
    "        self.act_count = np.zeros(self.k)               \n",
    "        self.iteration = 0     \n",
    "        if self.learning_rule == \"BayesianBetaPrior\":\n",
    "            self.alpha = np.random.uniform(size=self.k)\n",
    "            self.beta  = np.random.uniform(size=self.k)\n",
    "        if len(init_q) == self.k:\n",
    "            self.q_estimate = init_q\n",
    "        else:\n",
    "            raise Exception('Number of initial values ({}) does not correspond to number of arms ({}).'.format(len(init_q), self.k))\n",
    "\n",
    "    def learn(self, a, r):\n",
    "        # Guardo last_reward para el gradient bandits\n",
    "        if self.decision_rule == \"gradient\":\n",
    "            self.last_action = a\n",
    "            self.last_reward = r\n",
    "        \n",
    "        # dada una acción y una recompenza, actualiza la value function.\n",
    "        if self.learning_rule == \"averaging\":\n",
    "            self.q_estimate[a] += 1/self.act_count[a] * (r - self.q_estimate[a])\n",
    "\n",
    "        if self.learning_rule == \"BayesianBetaPrior\":\n",
    "            self.alpha[a] += r\n",
    "            self.beta[a]  += 1 - r \n",
    "            \n",
    "    def act(self):\n",
    "        # realiza una acción.\n",
    "        self.iteration += 1 \n",
    "        \n",
    "        if self.decision_rule == \"greedy\":\n",
    "            selected_action = greedyAction(self.q_estimate)\n",
    "            pass\n",
    "        if self.decision_rule == \"epsilon-greedy\":\n",
    "            selected_action = epsilonGreedyAction(self.q_estimate, self.epsilon)\n",
    "            pass\n",
    "        if self.decision_rule == \"UCB\":\n",
    "            selected_action = upperConfidenceBoundAction(self.q_estimate, self.UCB_param,\n",
    "                                                         self.iteration, self.act_count)\n",
    "            pass\n",
    "        if self.decision_rule == \"gradient\":\n",
    "            if self.iteration == 1:\n",
    "                self.pref = np.empty(self.k)\n",
    "                self.pref.fill(0)\n",
    "                self.last_reward = 0\n",
    "                self.last_action = 0\n",
    "            selected_action = gradientBanditAction(self.q_estimate, self.pref, self.lr,\n",
    "                                                   self.last_action, self.last_reward,\n",
    "                                                   self.iteration, self.k)\n",
    "        if self.decision_rule == \"Thompson\":\n",
    "            # COMPLETAR\n",
    "            pass\n",
    "        # Intento mantener q_estimate entre repes\n",
    "        #self.init_q = self.q_estimate\n",
    "        #print(self.init_q)\n",
    "        self.act_count[selected_action] += 1\n",
    "        return selected_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'softmax_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-95aa1b973c39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpi2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_vect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mindicactor2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindicactor2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpi2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'softmax_vect' is not defined"
     ]
    }
   ],
   "source": [
    "test = np.empty(2)\n",
    "test.fill(0)\n",
    "pi2 = softmax_vect(test)\n",
    "indicactor2 = np.array([1,0])\n",
    "test += 0.1*0.5*(indicactor2-pi2)\n",
    "print(\"0.1 * 0.5 *(\",indicactor2, \"-\", pi2, \") =\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all function definition cells before this one!\n",
    "#\n",
    "# Gradient Experiment\n",
    "params = {\"lr\": 0.3}\n",
    "agent = Agent(\"averaging\", \"gradient\", params)\n",
    "agents = np.array([agent])\n",
    "initp = np.array([0.4, 0.8]) #prob of each arm of giving reward\n",
    "narms = len(initp)\n",
    "initq = np.zeros(narms)\n",
    "plot_results(agents, actions=narms, rewards=initp)\n",
    "#it [ H preferences ] [ softmax probs ]  sel.action last.action last.reward r_diff indicactor q_estimate initq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1*(5==5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedyAction(q_estimate):\n",
    "    # Choose greedly one of the best actions\n",
    "    return randomArgmax(q_estimate)\n",
    "\n",
    "def epsilonGreedyAction(q_estimate, eps):\n",
    "    # Greedy action with prob p=1-eps, random with prob eps=1-p\n",
    "    if np.random.choice([True, False], p=[1-eps, eps]):\n",
    "        # Greedy action\n",
    "        return greedyAction(q_estimate)\n",
    "    else:\n",
    "        # Random action\n",
    "        return np.random.choice(np.arange(len(q_estimate)))\n",
    "\n",
    "def upperConfidenceBoundAction(q_estimate, c, iteration, act_count):\n",
    "    # De Sutton:\n",
    "    # \"if action a was never chosen > a is a maximizing action\"\n",
    "    # NO quiero que las acciones jamas elegidas 'me pisen' acciones con\n",
    "    # pesos fuertes por tener peso infinito al dividir por cero.\n",
    "    uncertainty = np.sqrt(np.log(iteration)/np.maximum(1, act_count))\n",
    "    # Con 'maximizing action' de Sutton, elijo su valor a partir del maximo de las\n",
    "    # acciones ya elegidas.\n",
    "    # ie: Las acciones no elegidas tienen EL MISMO valor que LA MEJOR accion elegida hasta el momento,\n",
    "    #     de esa forma evito que solo elija acciones NO tomadas hasta que las tome todas.\n",
    "    # 1. q_estimate + uncertainty para acciones ya elegidas (ignoro las NO elegidas)\n",
    "    part_candidates = q_estimate + c*np.where(act_count>0, uncertainty, 0)\n",
    "    # 2. le doy el max puntaje (de q_estimate) a las acciones NO elegidas\n",
    "    candidates = np.where(act_count>0, part_candidates, np.amax(part_candidates))\n",
    "    selected_action = randomArgmax(candidates)\n",
    "    return selected_action\n",
    "\n",
    "def gradientBanditAction(q_estimate, pref, lr, last_action, last_reward, iteration, arms):\n",
    "    # H (self.pref) funcion de preferencia que actualiza cuando compara\n",
    "    # el reward R que obtuve con mi reward promedio estimado R^\n",
    "    # Debo actualizar dos cosas:\n",
    "    #    la preferencia de la accion que tome (aumenta o disminuye)\n",
    "    #    la preferencia de las otras acciones (disminuyen o aumentan)\n",
    "    actions = np.arange(arms) \n",
    "    if iteration == 1:\n",
    "        # First action decision, without having rewards, so\n",
    "        # equal preferences for each action\n",
    "        #pref = np.empty(arms)\n",
    "        #pref.fill(0)\n",
    "        pass\n",
    "    else:\n",
    "        pi = softmax_vect(pref)\n",
    "        # Obs: uso q_estimate como promedio de rewards\n",
    "        r_diff = last_reward - q_estimate[last_action]\n",
    "        indicactor = np.where(actions==last_action, np.ones(arms), 0)\n",
    "        pref += lr * r_diff * (indicactor-pi)\n",
    "    pi = softmax_vect(pref)\n",
    "    selected_action = np.random.choice(actions, p=pi)\n",
    "    # set Verbose = True to print 'c' consecutives values every 'every' values:\n",
    "    verbose = False\n",
    "    c = 10\n",
    "    every = 100\n",
    "    if verbose and np.any(np.mod(c*[iteration], c*[every]) == np.arange(2,2+c)):\n",
    "        print(iteration,\"\\t\" , np.around(pi, 3),\"\\t\", \n",
    "              \"a_t+1:\"+str(selected_action),\"\\t\" , \"a:\"+str(last_action), \n",
    "              \"r:\"+str(last_reward), \"d:\"+str(np.around(r_diff, 3)),\"\\t\" , indicactor, np.around(q_estimate, 3))\n",
    "    return selected_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomArgmax(v):\n",
    "    \"\"\"Like np.argmax(), but if there are several \"best\" actions,\n",
    "       chooses and returns one randomly\"\"\"\n",
    "    arguments = np.argwhere(v == np.amax(v)).ravel()\n",
    "    arg = np.random.choice(arguments)\n",
    "    return arg\n",
    "\n",
    "def softmax_vect(v):\n",
    "    \"\"\"Returns a vector with all softmax values\"\"\"\n",
    "    exp_vect = np.exp(v)\n",
    "    return exp_vect/np.sum(exp_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Bandits\n",
    "\n",
    "Gradient bandits actualiza la preferencia de cada accion deacuerdo a \n",
    "la diferencia entre el reward obtenido y el promedio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gradient-bandits.png\" width=\"500\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateBandits(agents, narms, initp=None, initq=None, repetitions=1000, N=100):\n",
    "    # función que realiza las simulaciones de los agentes. Se define el número de repeticiones que seran\n",
    "    #  promediadas y el número de pasos N. agents es una lista de agentes.\n",
    "    rewards = np.zeros((len(agents), repetitions, N))\n",
    "    bestarm = np.zeros((len(agents), repetitions, N))\n",
    "    for i, agent in enumerate(agents):\n",
    "        for j in np.arange(repetitions):\n",
    "            environment = BernoulliBanditEnv(num_arms=narms, p=initp)\n",
    "            # Intento mantener q_estimate entre repeticiones\n",
    "            #print(initq)\n",
    "            agent.environment(environment, initq if not(np.array_equal(initq, None)) else np.zeros(narms))\n",
    "            #print(initq)\n",
    "            for n in np.arange(N):\n",
    "                a = agent.act()\n",
    "                r = environment.reward(a)\n",
    "                agent.learn(a, r)\n",
    "                rewards[i, j, n] = r\n",
    "                bestarm[i, j, n] = 1 if a == environment.best_action else 0\n",
    "    \n",
    "    return np.squeeze(np.mean(rewards, axis=1)), np.squeeze(np.mean(bestarm, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(agents, actions, rewards):\n",
    "    # Simulates and plot data\n",
    "    # Number of steps to play\n",
    "    N = 5000\n",
    "    # We play for 'N' steps, then repeat for 'reps' times, and take the mean\n",
    "    reps = 100\n",
    "    # Necesary for initq to be an array to be improved over iterations\n",
    "    # initq = np.zeros(actions)\n",
    "    # Generating data\n",
    "    total_data = np.asarray(simulateBandits(agents, narms=actions, initp=rewards, initq=initq, repetitions=reps, N=N))\n",
    "    # data shape: (rewards, agents, time step)\n",
    "    if len(agents)==1:\n",
    "        # reshaping as several actor data, so it matches dimensions\n",
    "        total_data = total_data.reshape(2,1,N)\n",
    "    # Defining plots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 15))\n",
    "    # Subplot 1: Rewards over time\n",
    "    ax1.set_title(\"Rewards over time\")\n",
    "    ax1.set_xlabel(\"time step\")\n",
    "    ax1.set_ylabel(\"reward\")\n",
    "    ax1.grid()\n",
    "    # Subplot 2: Agent's rate of choosing best arm\n",
    "    ax2.set_title(\"Rate of choosing best arm\")\n",
    "    ax2.set_xlabel(\"time step\")\n",
    "    ax2.set_ylabel(\"rate\")\n",
    "    ax2.grid()\n",
    "    # Subplot 3: Cumulative reward\n",
    "    ax3.set_title(\"Cumulative reward\")\n",
    "    ax3.set_xlabel(\"time step\")\n",
    "    ax3.set_ylabel(\"cumulative reward\")\n",
    "    ax3.grid()\n",
    "    time_steps = np.arange(1, N+1)\n",
    "    # Doing fancy stuff with colors\n",
    "    colors=iter(cm.Set1(np.linspace(0,1,len(agents))))\n",
    "    # looping over individual agent data\n",
    "    for a in range(total_data.shape[-2]):\n",
    "        # data from each agent\n",
    "        data = total_data[:,a,:]\n",
    "        # Subplot 1: Rewards over time\n",
    "        #ax1.plot(time_steps, data[0,:], 'r.-', label=\"reward-actor-\"+str(a), linewidth=0.2)\n",
    "        color = next(colors)\n",
    "        ax1.plot(time_steps, data[0,:], marker=\".\", linestyle='-', linewidth=0.2,\n",
    "                 color=color, label=\"reward-actor-\"+str(a+1))\n",
    "        ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        # Subplot 2: Agent's rate of choosing best arm\n",
    "        ax2.plot(time_steps, data[1,:], marker=\".\", linestyle='-', linewidth=0.2,\n",
    "                 color=color, label=\"best-arm-actor-\"+str(a+1))\n",
    "        ax2.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        # Subplot 3: Cumulative reward\n",
    "        ax3.plot(time_steps, np.cumsum(data[1,:]), marker=\".\", markersize=1, linestyle='-', linewidth=0.5,\n",
    "                 color=color,\n",
    "                 label=\"cumulative-actor-\"+str(a+1))\n",
    "        # black straight line from first to last value to see curvatures\n",
    "        ax3.plot([time_steps[0], time_steps[-1]],\n",
    "                 [np.cumsum(data[1,:])[0], np.cumsum(data[1,:])[-1]],\n",
    "                 'ko-', label=\"straight-line-\"+str(a), markersize=4, linewidth=0.5)\n",
    "        ax3.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[go down](#down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios:\n",
    "\n",
    "1. Completar pertinentemente el código donde diga \"COMPLETAR\".\n",
    "\n",
    "[2.](#Bandits-de-2-brazos) Realizar simulaciones con un bandit de 2 brazos (P = [0.4, 0.8]) para cada una de las reglas de decisión y graficar la recompensa promedio, la recompensa acumulada y el porcentaje de veces que fue elegido el mejor brazo en función de los pasos. Interprete los resultados.\n",
    "\n",
    "[3.](#Bandits-de-10-brazos) Realizar simulaciones con un bandit de 10 brazos (P = [0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.2]) para cada una de las reglas de decisión y graficar la recompensa promedio, la recompensa acumulada y el porcentaje de veces que fue elegido el mejor brazo en función de los pasos. Interprete los resultados.\n",
    "\n",
    "[4.](#Estudio-de-Epsilon-Greedy) Estudie la dependencia del hiperparametro epsilon en la regla de decisión **epsilon-greedy**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Completar\n",
    "\n",
    "#### Hecho en celdas superiores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bandits de 2 brazos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\"averaging\", \"greedy\")\n",
    "agents = np.array([agent])\n",
    "initp = np.array([0.4, 0.8]) #prob of each arm of giving reward\n",
    "narms = len(initp)\n",
    "initq = np.zeros(narms)\n",
    "plot_results(agents, actions=narms, rewards=initp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"epsilon\": 0.2}\n",
    "agent = Agent(\"averaging\", \"epsilon-greedy\", params)\n",
    "agents = np.array([agent])\n",
    "initp = np.array([0.4, 0.8]) #prob of each arm of giving reward\n",
    "narms = len(initp)\n",
    "initq = np.zeros(narms)\n",
    "plot_results(agents, actions=narms, rewards=initp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"UCB_param\": 0.1}\n",
    "agent = Agent(\"averaging\", \"UCB\", params)\n",
    "agents = np.array([agent])\n",
    "initp = np.array([0.4, 0.8]) #prob of each arm of giving reward\n",
    "narms = len(initp)\n",
    "initq = np.zeros(narms)\n",
    "plot_results(agents, actions=narms, rewards=initp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"lr\": 0.1}\n",
    "agent = Agent(\"averaging\", \"gradient\", params)\n",
    "agents = np.array([agent])\n",
    "initp = np.array([0.4, 0.8]) #prob of each arm of giving reward\n",
    "narms = len(initp)\n",
    "initq = np.zeros(narms)\n",
    "plot_results(agents, actions=narms, rewards=initp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bandits de 10 brazos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\"averaging\", \"greedy\")\n",
    "agents = np.array([agent])\n",
    "initp = np.array([0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.2]) #prob of each arm of giving reward\n",
    "narms = len(initp)\n",
    "initq = np.zeros(narms)\n",
    "plot_results(agents, actions=narms, rewards=initp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"epsilon\": 0.01}\n",
    "agent = Agent(\"averaging\", \"epsilon-greedy\", params)\n",
    "agents = np.array([agent])\n",
    "initp = np.array([0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.2]) #prob of each arm of giving reward\n",
    "narms = len(initp)\n",
    "initq = np.zeros(narms)\n",
    "plot_results(agents, actions=narms, rewards=initp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"UCB_param\": 0.1}\n",
    "agent = Agent(\"averaging\", \"UCB\", params)\n",
    "agents = np.array([agent])\n",
    "initp = np.array([0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.2]) #prob of each arm of giving reward\n",
    "narms = len(initp)\n",
    "initq = np.zeros(narms)\n",
    "plot_results(agents, actions=narms, rewards=initp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"lr\": 0.01}\n",
    "agent = Agent(\"averaging\", \"gradient\", params)\n",
    "agents = np.array([agent])\n",
    "initp = np.array([0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.2]) #prob of each arm of giving reward\n",
    "narms = len(initp)\n",
    "initq = np.zeros(narms)\n",
    "plot_results(agents, actions=narms, rewards=initp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with several agent data on same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = {\"UCB_param\": 0.1}\n",
    "agent1 = Agent(\"averaging\", \"UCB\", params1)\n",
    "params2 = {\"UCB_param\": 1.4}\n",
    "agent2 = Agent(\"averaging\", \"UCB\", params2)\n",
    "params3 = {\"UCB_param\": 5}\n",
    "agent3 = Agent(\"averaging\", \"UCB\", params3)\n",
    "#\n",
    "agents = np.array([agent1, agent2, agent3])\n",
    "initp = np.array([0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.2]) #prob of each arm of giving reward\n",
    "narms = len(initp)\n",
    "initq = np.zeros(narms)\n",
    "plot_results(agents, actions=narms, rewards=initp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Experiment\n",
    "params1 = {\"lr\": 0.1}\n",
    "agent1 = Agent(\"averaging\", \"gradient\", params1)\n",
    "params2 = {\"lr\": 0.2}\n",
    "agent2 = Agent(\"averaging\", \"gradient\", params2)\n",
    "params3 = {\"lr\": 0.3}\n",
    "agent3 = Agent(\"averaging\", \"gradient\", params3)\n",
    "params4 = {\"lr\": 0.4}\n",
    "agent4 = Agent(\"averaging\", \"gradient\", params4)\n",
    "params5 = {\"lr\": 0.5}\n",
    "agent5 = Agent(\"averaging\", \"gradient\", params5)\n",
    "params6 = {\"lr\": 0.6}\n",
    "agent6 = Agent(\"averaging\", \"gradient\", params6)\n",
    "#\n",
    "agents = np.array([agent1, agent2, agent3, agent4, agent5, agent6])\n",
    "initp = np.array([0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.2]) #prob of each arm of giving reward\n",
    "narms = len(initp)\n",
    "initq = np.zeros(narms)\n",
    "plot_results(agents, actions=narms, rewards=initp)\n",
    "#it [ H preferences ] [ softmax probs ]  sel.action last.action last.reward r_diff "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[go up](#up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Estudio de Epsilon-Greedy\n",
    "#### y la dependencia de epsilon en la regla de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = {\"epsilon\": 0.01}\n",
    "agent1 = Agent(\"averaging\", \"epsilon-greedy\", params1)\n",
    "params2 = {\"epsilon\": 0.05}\n",
    "agent2 = Agent(\"averaging\", \"epsilon-greedy\", params2)\n",
    "params3 = {\"epsilon\": 0.1}\n",
    "agent3 = Agent(\"averaging\", \"epsilon-greedy\", params3)\n",
    "params4 = {\"epsilon\": 0.5}\n",
    "agent4 = Agent(\"averaging\", \"epsilon-greedy\", params4)\n",
    "params5 = {\"epsilon\": 1}\n",
    "agent5 = Agent(\"averaging\", \"epsilon-greedy\", params5)\n",
    "agents = np.array([agent1, agent2, agent3, agent4, agent5])\n",
    "initp = np.array([0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.2]) #prob of each arm of giving reward\n",
    "narms = len(initp)\n",
    "initq = np.zeros(narms)\n",
    "plot_results(agents, actions=narms, rewards=initp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
