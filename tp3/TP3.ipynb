{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3: Model Free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción del código: \n",
    "\n",
    "En este Trabajo se les presenta un juego de mesa (una versión simplificada del Perudo: https://www.youtube.com/watch?v=die0n-eonl8, juego que aparece en la pelicula Piratas Del Caribe 2: El cofre de la Muerte). A partir de las reglas de dicho juego se construye un environment.\n",
    "\n",
    "Además se les brinda un breve código en donde se juega contra dicho environment de forma aleatoria.\n",
    "\n",
    "Por ultimo, también encontraran dos funciones para graficar la Value function y una política respectivamente.\n",
    "\n",
    "### Consignas:\n",
    "\n",
    "1) Implementar el método de Monte Carlo, SARSA y Q-learning para aprender la función valor. Nota: no es necesario, pero si recomendable, usar el esqueleto de funciones que aparecen abajo.\n",
    "\n",
    "2) Para cada uno de los 3 casos, graficar dicha función valor y la política optima encontrada.\n",
    "\n",
    "3) Crear un nuevo environment que tome como parámetro una política (dicho parámetro deberá ser la política realizada por la IA). Luego realizar iteraciones para cualquiera de los 3 métodos mencionados, en donde para cada iteración encuentre una política óptima para un jugador que se enfrenta contra la política óptima del paso anterior. Nuevamente graficar la función valor y la política optima final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.ytimg.com/vi/gMGsAxuWozQ/maxresdefault.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://i.ytimg.com/vi/gMGsAxuWozQ/maxresdefault.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from gym import spaces\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El juego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reglas:\n",
    "\n",
    "- 2 jugadores (uno sos vos y una IA)\n",
    "- Cada jugador cuenta con 5 monedas (cara o cruz). Cada jugador solo ve sus monedas. \n",
    "- Después de tirar las monedas, empieza el juego.\n",
    "- El juego consiste en intentar adivinar al menos cuantas caras hay en total entre todas las monedas o en hacer que el rival adivine incorrectamente.\n",
    "- La apuesta empieza en 0 caras.\n",
    "- Se tira una moneda para ver quien de los dos jugadores empieza. \n",
    "- Acciones posibles:\n",
    "    * me quedo con la apuesta hasta ahora y paso.\n",
    "    * subo en 1 la cantidad de caras que creo que hay en total.\n",
    "- El juego termina cuando algún jugador pasa.\n",
    "- Si la apuesta fue mayor al numero real de caras, el agostador pierde (r=-1) y el otro gana. Si la apuesta es menor o igual al numero real, gana el apostados (r=1) y pierde el otro.\n",
    "- IA inicial de la computadora: Si la apuesta es menor a 2 + cantidad de caras propias, la computadora apuesta. Sino pasa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino nuestro propio environment con las reglas del juego\n",
    "\n",
    "def throw_coin(num_coin, np_random):\n",
    "    return np_random.rand(num_coin)>0.5\n",
    "\n",
    "def total_faces(list_players):\n",
    "    RV=0\n",
    "    for player in list_players:\n",
    "        RV += sum(player)\n",
    "    return RV\n",
    "\n",
    "class PerudoSimplificado(gym.Env):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(5), #mis monedas\n",
    "            spaces.Discrete(10))) #apuesta actual\n",
    "\n",
    "        self.seed()\n",
    "        # Empieza el juego\n",
    "        self.reset()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        \n",
    "        max_guess_player_2 = total_faces([self.player_2]) + int(len(self.player_1)/2)\n",
    "        faces_tot = total_faces([self.player_1, self.player_2])\n",
    "        \n",
    "        if self.guess > len(self.player_1) + len(self.player_2): #la apuesta es mayor que lo maximo posible\n",
    "            done = True\n",
    "            reward = -1\n",
    "\n",
    "        if action == 0: #action == 0, mantengo la apuesta y paso\n",
    "            done = True\n",
    "            if self.guess <= faces_tot: #tenia razon el rival\n",
    "                reward = -1\n",
    "            else: #tenia razon yo\n",
    "                reward = -1\n",
    "            \n",
    "        else: #action == 1, subo en 1 la apuesta\n",
    "            self.guess += 1\n",
    "            if self.guess < max_guess_player_2: #el rival sube 1 en su turno\n",
    "                self.guess += 1 \n",
    "                done = False\n",
    "                reward = 0\n",
    "            else: # el rival pasa\n",
    "                done = True\n",
    "                if self.guess <= faces_tot: \n",
    "                    reward = 1\n",
    "                else:\n",
    "                    reward = -1\n",
    "            \n",
    "        return self.get_obs(), reward, done, {}\n",
    "\n",
    "    def get_obs(self):\n",
    "        return (sum(self.player_1), self.guess)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.player_1 = throw_coin(5,self.np_random)\n",
    "        self.player_2 = throw_coin(5,self.np_random)\n",
    "        self.guess = 1 if np.random.rand()>0.5 else 0 #al principio se tira una moneda para ver quien empieza. Por eso es 0 (uno empieza) o 1 (empezo el otro y subio)  \n",
    "        return self.get_obs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jugando al azar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PerudoSimplificado()\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Politica Random:\n",
    "for i_episode in range(5):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = env.action_space.sample() # Selecciona una accion random dentro de las posibles\n",
    "        state, reward, done, info = env.step(action) # Juega un paso\n",
    "        print(state,action)\n",
    "        if done:\n",
    "            print('El juego ha terminado! Su reward: ', reward)\n",
    "            print('Ganaste :)\\n') if reward > 0 else print('Perdiste :(\\n')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graficos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def plot_values(V):\n",
    "    \"\"\"\n",
    "    Realiza un grafico en 3 dimensiones de la función valor.\n",
    "        \n",
    "    El parametro V es un diccionario cuyas keys son pares \"caras en mi mano\" \n",
    "    y \"apuesta actual\".\n",
    "    \"\"\"\n",
    "    def get_Z(x, y):\n",
    "        if (x,y) in V:\n",
    "            return V[x,y]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def get_figure(ax):\n",
    "        x_range = np.arange(0, 6)\n",
    "        y_range = np.arange(1, 11)\n",
    "        X, Y = np.meshgrid(x_range, y_range)\n",
    "        \n",
    "        Z = np.array([get_Z(x,y) for x,y in zip(np.ravel(X), np.ravel(Y))]).reshape(X.shape)\n",
    "\n",
    "        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
    "        ax.set_xlabel('caras en tu mano')\n",
    "        ax.set_ylabel('apuestas')\n",
    "        ax.set_zlabel('value')\n",
    "        ax.view_init(ax.elev, -120)\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    ax = fig.add_subplot(211, projection='3d')\n",
    "    get_figure(ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy(policy):\n",
    "    \"\"\"\n",
    "    Realiza un grafico en 3 dimensiones de la función valor.\n",
    "    \n",
    "    El parametro policy es un diccionario cuyas keys son pares \"caras en mi mano\" \n",
    "    y \"apuesta actual\" y el valor es la acción ha realizar.\n",
    "    \"\"\"\n",
    "    def get_Z(x, y):\n",
    "        if (x,y) in policy:\n",
    "            return policy[x,y]\n",
    "        else:\n",
    "            return 25 # este valor \"25\" es para visualizar que la policy no tiene asignada una acción para dicho estado.\n",
    "\n",
    "    def get_figure( ax):\n",
    "        x_range = np.arange(0, 6)\n",
    "        y_range = np.arange(0, 11)\n",
    "        X, Y = np.meshgrid(x_range, y_range)\n",
    "        Z = np.array([[get_Z(x,y) for x in x_range] for y in y_range])\n",
    "        surf = ax.imshow(np.flip(Z,0), cmap=plt.get_cmap('Pastel2', 3), vmin=0, vmax=2, extent=[-0.5, 5.5, -0.5, 10.5])\n",
    "        plt.xticks(x_range)\n",
    "        plt.yticks(y_range)\n",
    "        plt.gca().invert_yaxis()\n",
    "        ax.set_xlabel('caras en mi mano')\n",
    "        ax.set_ylabel('apuestas')\n",
    "        ax.grid(color='w', linestyle='-', linewidth=1)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "        cbar = plt.colorbar(surf, ticks=[0,1,2], cax=cax)\n",
    "        cbar.ax.set_yticklabels(['0 (pasar)','1 (subir)', 'desconocido'])\n",
    "        print(Z)\n",
    "            \n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    get_figure(ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(Q_s, epsilon, nA): \n",
    "    # Completar\n",
    "    return policy_s\n",
    "  \n",
    "\n",
    "def generate_episode_from_Q(env, Q, epsilon, nA):\n",
    "    # Completar\n",
    "    return episode\n",
    "\n",
    "\n",
    "def update_Q(env, episode, Q, alpha, gamma):\n",
    "    # Completar\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes, alpha, gamma=1.0, eps_start=1.0, eps_decay=.99999, eps_min=0.05):\n",
    "    # Completar\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la politica optima y el value function\n",
    "policy, Q = mc_control(env, 500000,0.015)\n",
    "V = dict((k,np.max(v)) for k, v in Q.items())\n",
    "plot_values(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotear la politica\n",
    "plot_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q_sarsa(alpha, gamma, Q, state, action, reward, next_state=None, next_action=None):\n",
    "    # Completar\n",
    "    \n",
    "def epsilon_greedy(Q, state, nA, eps):\n",
    "    # Completar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, alpha, gamma=1.0, epsmin=0.01):\n",
    "    # Completar\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la politica optima y el value function\n",
    "Q_sarsa = sarsa(env, 500000, 0.009)\n",
    "V = dict((k,np.max(v)) for k, v in Q_sarsa.items())\n",
    "plot_values(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotear la politica\n",
    "policy_sarsa = dict((k,np.argmax(v)) for k, v in Q_sarsa.items())\n",
    "plot_policy(policy_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q_sarsamax(alpha, gamma, Q, state, action, reward, next_state=None):\n",
    "    # Completar\n",
    "    return new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, alpha, gamma=1.0,epsmin=0.01):\n",
    "    # Completar\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la politica optima y el value function\n",
    "Q_sarsamax = q_learning(env, 500000, 0.01)\n",
    "V = dict((k,np.max(v)) for k, v in Q_sarsamax.items())\n",
    "plot_values(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotear la politica\n",
    "policy_sarsamax = dict((k,np.argmax(v)) for k, v in Q_sarsamax.items())\n",
    "plot_policy(policy_sarsamax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
