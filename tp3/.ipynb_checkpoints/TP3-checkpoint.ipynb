{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3: Model Free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción del código: \n",
    "\n",
    "En este Trabajo se les presenta un juego de mesa (una versión simplificada del Perudo: https://www.youtube.com/watch?v=die0n-eonl8, juego que aparece en la pelicula Piratas Del Caribe 2: El cofre de la Muerte). A partir de las reglas de dicho juego se construye un environment.\n",
    "\n",
    "Además se les brinda un breve código en donde se juega contra dicho environment de forma aleatoria.\n",
    "\n",
    "Por ultimo, también encontraran dos funciones para graficar la Value function y una política respectivamente.\n",
    "\n",
    "### Consignas:\n",
    "\n",
    "1. Implementar el método de Monte Carlo, SARSA y Q-learning para aprender la función valor.\n",
    "\n",
    " Nota: no es necesario, pero si recomendable, usar el esqueleto de funciones que aparecen abajo.\n",
    " \n",
    "\n",
    "2. Para cada uno de los 3 casos, graficar dicha función valor y la política optima encontrada.\n",
    "\n",
    "\n",
    "3. Crear un nuevo environment que tome como parámetro una política (dicho parámetro deberá ser la política realizada por la IA).\n",
    "\n",
    " Luego realizar iteraciones para cualquiera de los 3 métodos mencionados, en donde para cada iteración encuentre una política óptima para un jugador que se enfrenta contra la política óptima del paso anterior.\n",
    "\n",
    " Nuevamente graficar la función valor y la política optima final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:58:17.432127Z",
     "start_time": "2019-05-30T13:58:17.423943Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://i.ytimg.com/vi/gMGsAxuWozQ/maxresdefault.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:58:17.780595Z",
     "start_time": "2019-05-30T13:58:17.575529Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from gym import spaces\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El juego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reglas:\n",
    "\n",
    "- 2 jugadores (uno sos vos y una IA)\n",
    "- Cada jugador cuenta con **5 monedas** (cara o cruz). Cada jugador solo ve sus monedas. \n",
    "- Después de tirar las monedas, empieza el juego.\n",
    "- El juego consiste en intentar adivinar al menos **cuantas caras hay en total** entre todas las monedas o en hacer que el **rival adivine incorrectamente**.\n",
    "- La apuesta empieza en 0 caras.\n",
    "- Se tira una moneda para ver quien de los dos jugadores empieza. \n",
    "- Acciones posibles:\n",
    "    * me quedo con la apuesta hasta ahora y paso.\n",
    "    * subo en 1 la cantidad de caras que creo que hay en total.\n",
    "- El juego termina cuando algún jugador pasa.\n",
    "- Si la apuesta fue **mayor** al numero real de caras, **el apostador pierde** (r=-1) y el otro gana.\n",
    "- Si la apuesta es **menor o igual** al numero real, **gana el apostador** (r=1) y pierde el otro.\n",
    "- IA inicial de la computadora: Si la apuesta es menor a 2 + cantidad de caras propias, la computadora apuesta. Sino pasa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:58:18.148224Z",
     "start_time": "2019-05-30T13:58:18.141913Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defino nuestro propio environment con las reglas del juego\n",
    "\n",
    "def throw_coin(num_coin, np_random):\n",
    "    return np_random.rand(num_coin)>0.5\n",
    "\n",
    "def total_faces(list_players):\n",
    "    RV=0\n",
    "    for player in list_players:\n",
    "        RV += sum(player)\n",
    "    return RV\n",
    "\n",
    "class PerudoSimplificado(gym.Env):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(5), #mis monedas\n",
    "            spaces.Discrete(10))) #apuesta actual\n",
    "\n",
    "        self.seed()\n",
    "        # Empieza el juego\n",
    "        self.reset()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        \n",
    "        max_guess_player_2 = total_faces([self.player_2]) + int(len(self.player_1)/2)\n",
    "        faces_tot = total_faces([self.player_1, self.player_2])\n",
    "        \n",
    "        if self.guess > len(self.player_1) + len(self.player_2): #la apuesta es mayor que lo maximo posible\n",
    "            done = True\n",
    "            reward = -1\n",
    "\n",
    "        if action == 0: #action == 0, mantengo la apuesta y paso\n",
    "            done = True\n",
    "            if self.guess <= faces_tot: #tenia razon el rival\n",
    "                reward = -1\n",
    "            else: #tenia razon yo\n",
    "                reward = +1\n",
    "            \n",
    "        else: #action == 1, subo en 1 la apuesta\n",
    "            self.guess += 1\n",
    "            if self.guess < max_guess_player_2: #el rival sube 1 en su turno\n",
    "                self.guess += 1 \n",
    "                done = False\n",
    "                reward = 0\n",
    "            else: # el rival pasa\n",
    "                done = True\n",
    "                if self.guess <= faces_tot: \n",
    "                    reward = 1\n",
    "                else:\n",
    "                    reward = -1\n",
    "            \n",
    "        return self.get_obs(), reward, done, {}\n",
    "\n",
    "    def get_obs(self):\n",
    "        return (sum(self.player_1), self.guess)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.player_1 = throw_coin(5,self.np_random)\n",
    "        self.player_2 = throw_coin(5,self.np_random)\n",
    "        self.guess = 1 if np.random.rand()>0.5 else 0 #al principio se tira una moneda para ver quien empieza. Por eso es 0 (uno empieza) o 1 (empezo el otro y subio)  \n",
    "        return self.get_obs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jugando al azar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:58:18.452673Z",
     "start_time": "2019-05-30T13:58:18.445395Z"
    }
   },
   "outputs": [],
   "source": [
    "env = PerudoSimplificado()\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:58:18.726675Z",
     "start_time": "2019-05-30T13:58:18.722499Z"
    }
   },
   "outputs": [],
   "source": [
    "#Politica Random:\n",
    "for i_episode in range(1):\n",
    "    state = env.reset()\n",
    "    print(\"Starting at state:\", state)\n",
    "    while True:\n",
    "        #action = env.action_space.sample() # Selecciona una accion random dentro de las posibles\n",
    "        action = 1\n",
    "        state, reward, done, info = env.step(action) # Juega un paso\n",
    "        print(state, action)\n",
    "        if done:\n",
    "            print('El juego ha terminado! Su reward: ', reward)\n",
    "            print('Ganaste :)\\n') if reward > 0 else print('Perdiste :(\\n')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graficos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:58:19.240568Z",
     "start_time": "2019-05-30T13:58:19.022464Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def plot_values(V):\n",
    "    \"\"\"\n",
    "    Realiza un grafico en 3 dimensiones de la función valor.\n",
    "        \n",
    "    El parametro V es un diccionario cuyas keys son pares \"caras en mi mano\" \n",
    "    y \"apuesta actual\".\n",
    "    \"\"\"\n",
    "    def get_Z(x, y):\n",
    "        if (x,y) in V:\n",
    "            return V[x,y]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def get_figure(ax):\n",
    "        x_range = np.arange(0, 6)\n",
    "        y_range = np.arange(1, 11)\n",
    "        X, Y = np.meshgrid(x_range, y_range)\n",
    "        \n",
    "        Z = np.array([get_Z(x,y) for x,y in zip(np.ravel(X), np.ravel(Y))]).reshape(X.shape)\n",
    "\n",
    "        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
    "        ax.set_xlabel('caras en tu mano')\n",
    "        ax.set_ylabel('apuestas')\n",
    "        ax.set_zlabel('value')\n",
    "        ax.view_init(ax.elev, -120)\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    ax = fig.add_subplot(211, projection='3d')\n",
    "    get_figure(ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T13:58:19.247841Z",
     "start_time": "2019-05-30T13:58:19.242315Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_policy(policy):\n",
    "    \"\"\"\n",
    "    Realiza un grafico en 3 dimensiones de la función valor.\n",
    "    \n",
    "    El parametro policy es un diccionario cuyas keys son pares \"caras en mi mano\" \n",
    "    y \"apuesta actual\" y el valor es la acción a realizar.\n",
    "    \"\"\"\n",
    "    def get_Z(x, y):\n",
    "        if (x,y) in policy:\n",
    "            return policy[x,y]\n",
    "        else:\n",
    "            return 25 # este valor \"25\" es para visualizar que la policy no tiene asignada una acción para dicho estado.\n",
    "\n",
    "    def get_figure( ax):\n",
    "        x_range = np.arange(0, 6)\n",
    "        y_range = np.arange(0, 11)\n",
    "        X, Y = np.meshgrid(x_range, y_range)\n",
    "        Z = np.array([[get_Z(x,y) for x in x_range] for y in y_range])\n",
    "        surf = ax.imshow(np.flip(Z,0), cmap=plt.get_cmap('Pastel2', 3), vmin=0, vmax=2, extent=[-0.5, 5.5, -0.5, 10.5])\n",
    "        plt.xticks(x_range)\n",
    "        plt.yticks(y_range)\n",
    "        plt.gca().invert_yaxis()\n",
    "        ax.set_xlabel('caras en mi mano')\n",
    "        ax.set_ylabel('apuestas')\n",
    "        ax.grid(color='w', linestyle='-', linewidth=1)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "        cbar = plt.colorbar(surf, ticks=[0,1,2], cax=cax)\n",
    "        cbar.ax.set_yticklabels(['0 (pasar)','1 (subir)', 'desconocido'])\n",
    "        print(Z)\n",
    "            \n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    get_figure(ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T14:19:34.073731Z",
     "start_time": "2019-05-30T14:19:34.068234Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Filling these ones was freaking fun!\n",
    "\n",
    "def get_probs(Q_s, epsilon, nA): \n",
    "    # Completar\n",
    "    # Devuelve un array con las probs de cada accion, para un estado dado\n",
    "    # policy:   {(s,s): (r_a1, r_a2)}\n",
    "    # policy_s: [r_a1, r_a2]\n",
    "    policy_s = np.zeros(nA)\n",
    "    A_star = randomArgmax(Q_s)\n",
    "    actions = np.arange(nA)\n",
    "    for a in actions:\n",
    "        if a == A_star:\n",
    "            policy_s[a] = 1-epsilon + epsilon/nA\n",
    "        else:\n",
    "            policy_s[a] = epsilon/nA\n",
    "    \n",
    "#     if np.random.choice([1,0], p=[1-epsilon, epsilon]):\n",
    "#         # Returning policy from Q\n",
    "#         policy_s = Q_s\n",
    "#     else:\n",
    "#         # Returning random policy\n",
    "#         policy_s = np.ones(len(Q_s))/nA\n",
    "    print(\"policy_s\", policy_s)\n",
    "    return policy_s\n",
    "  \n",
    "def generate_episode_from_Q(env, Q, epsilon, nA):\n",
    "    # Completar\n",
    "    #episode = {(s,a):r}\n",
    "    episode = {}\n",
    "    state = env.reset()\n",
    "    actions = np.arange(nA)\n",
    "    while True:\n",
    "        action = np.random.choice(actions, p=get_probs(Q[state], epsilon, nA))\n",
    "        state, reward, done, info = env.step(action) # Juega un paso\n",
    "        \n",
    "        if (state, action) not in episode:\n",
    "            # First Visit MC.\n",
    "            episode[(state, action)] = reward\n",
    "        #print(state, action)\n",
    "        if done:\n",
    "#             print('El juego ha terminado! Su reward: ', reward)\n",
    "#             print('Ganaste :)\\n') if reward > 0 else print('Perdiste :(\\n')\n",
    "            break\n",
    "    return episode\n",
    "\n",
    "#def update_Q(env, episode, Q, alpha, gamma):\n",
    "def update_Q(env, episode, Q, alpha, gamma, Returns):\n",
    "    # Completar\n",
    "    nA=2\n",
    "    #TODO: ver si se puede hacer sin ese parametro Returns\n",
    "    # Mete en returns los valores de episode\n",
    "    #{ k: Returns.get(k, 0) + Q.get(k, 0) for k in set(Returns) | set(Q) }\n",
    "    #\n",
    "    for (key) in episode:\n",
    "        # Adding episode first values to list of past Returns\n",
    "        print(\"before append: episode[key]\", episode[key], \"- Returns[key]\", Returns[key])\n",
    "        Returns[key] = np.append(Returns[key], episode[key])\n",
    "        print(\"after  append: episode[key]\", episode[key], \"- Returns[key]\", Returns[key])\n",
    "        # Now Q(s,a) takes the mean of Returns(s,a)\n",
    "        Q[key] = np.mean(Returns[key])\n",
    "        \n",
    "        print(\"episode[key]\", episode[key], \"- Q[key]\", Q[key], \"- Returns[key]\", Returns[key])\n",
    "    #print(\"Returns: \",Returns)\n",
    "    #print(\"Q: \", Q)\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T14:19:34.314677Z",
     "start_time": "2019-05-30T14:19:34.311211Z"
    }
   },
   "outputs": [],
   "source": [
    "D = {'a':1, 'b':2, 'c':3}\n",
    "D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T14:19:34.522950Z",
     "start_time": "2019-05-30T14:19:34.520084Z"
    }
   },
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T14:19:34.717016Z",
     "start_time": "2019-05-30T14:19:34.713766Z"
    }
   },
   "outputs": [],
   "source": [
    "randomArgmax((1,2,3,4,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T14:19:34.884056Z",
     "start_time": "2019-05-30T14:19:34.881065Z"
    }
   },
   "outputs": [],
   "source": [
    "def randomArgmax(v):\n",
    "    \"\"\"Like np.argmax(), but if there are several \"best\" actions,\n",
    "       chooses and returns one randomly\"\"\"\n",
    "    arguments = np.argwhere(v == np.amax(v)).ravel()\n",
    "    arg = np.random.choice(arguments)\n",
    "    return arg\n",
    "\n",
    "def softmax_vect(v):\n",
    "    \"\"\"Returns a vector with all softmax values\"\"\"\n",
    "    exp_vect = np.exp(v)\n",
    "    return exp_vect/np.sum(exp_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T14:19:35.091540Z",
     "start_time": "2019-05-30T14:19:35.088263Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_QPol_est(D, default_value, nA):\n",
    "    # states: (c, a) \n",
    "    #           for all c in [0,5]\n",
    "    #           for all a in [0,10]\n",
    "    # actions: (v0, v1) < (0.5, 0.5)\n",
    "    # Q = {(c,a):(v0,v1),...,(d,b):(v0,v1)}\n",
    "    #armo todos los posibles indices, tal vez despues no haga falta\n",
    "    states = np.array(np.meshgrid(np.arange(0, 6 ), np.arange(0, 11))).T.reshape(-1,2)\n",
    "    keystates = tuple(map(tuple, states))\n",
    "    return dict.fromkeys(keystates, default_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T14:19:35.230004Z",
     "start_time": "2019-05-30T14:19:35.226388Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_Returns(D, default_value, nA):\n",
    "    # Returns = {((s,s),a):(v0,v1,v2, ... ,((d,b),a):(v0)}\n",
    "    #armo todos los posibles indices, tal vez despues no haga falta\n",
    "    states = np.array(np.meshgrid(np.arange(0, 6 ), np.arange(0, 11))).T.reshape(-1,2)\n",
    "    actions = np.arange(nA)\n",
    "    keysD = ()\n",
    "    for a in range(nA):\n",
    "        keysD += tuple(map(lambda s: (tuple(s), a), states))\n",
    "    return dict.fromkeys(keysD, default_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T14:19:35.425768Z",
     "start_time": "2019-05-30T14:19:35.421560Z"
    }
   },
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes, alpha, gamma=1.0, eps_start=1.0, eps_decay=.99999, eps_min=0.05):\n",
    "    # Completar\n",
    "    \"\"\"\n",
    "    Q es un diccionario cuyas keys son pares \n",
    "    <\"caras en mi mano\" y \"apuesta actual\"> (state) \n",
    "    <y el valor de CADA acción ha realizar >(action-value)\n",
    "    Q{(st, ate): (val, ue)}\n",
    "    \"\"\"\n",
    "    nA = 2\n",
    "    Q = {}\n",
    "    policy = {}\n",
    "    # se podra hacer sin inicializar Q para TODO s,a?\n",
    "    Q=initialize_QPol_est(Q, tuple(np.ones(nA)-1), nA)\n",
    "    policy = initialize_QPol_est(policy, tuple(np.ones(nA)/nA), nA)\n",
    "    #solo la primera vez! ver si se puede hacer update gradual para no usar Returns\n",
    "    Returns = {}\n",
    "    default_value = np.array([0])\n",
    "    Returns = initialize_Returns(Returns, default_value, nA)\n",
    "    #\n",
    "    for iteration in range(num_episodes):\n",
    "        epsilon = eps_start\n",
    "        episode = generate_episode_from_Q(env, Q, epsilon, nA)\n",
    "        #print(\"episode: \", episode)\n",
    "\n",
    "        Q = update_Q(env, episode, Q, alpha, gamma, Returns)\n",
    "\n",
    "        for (state, action) in episode:\n",
    "            policy[state] += get_probs(Q[state], epsilon, nA)\n",
    "    \n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/on-policy-first-visit-mc-control.png\" width=\"500\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T14:34:19.682154Z",
     "start_time": "2019-05-30T14:34:01.675904Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calcular la politica optima y el value function\n",
    "#policy, Q = mc_control(env, 500000, 0.015)\n",
    "policy, Q = mc_control(env, 5000, 0.015)\n",
    "V = dict((k, np.max(v)) for k, v in Q.items())\n",
    "plot_values(V)\n",
    "#Deberia hacer que Returns tenga como key SOLO el estado (s,s) y para cada accion, una lista de valores historicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T14:20:37.573989Z",
     "start_time": "2019-05-30T14:20:37.434941Z"
    }
   },
   "outputs": [],
   "source": [
    "# plotear la politica\n",
    "plot_policy(policy)\n",
    "# Para Q y policy use diccionarios y una tupla del otro lado\n",
    "# TODO: Cambiar la tupla por el valor de la accion que toma, al menos para policy, para Q NO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q_sarsa(alpha, gamma, Q, state, action, reward, next_state=None, next_action=None):\n",
    "    # Completar\n",
    "    \n",
    "def epsilon_greedy(Q, state, nA, eps):\n",
    "    # Completar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, alpha, gamma=1.0, epsmin=0.01):\n",
    "    # Completar\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la politica optima y el value function\n",
    "Q_sarsa = sarsa(env, 500000, 0.009)\n",
    "V = dict((k,np.max(v)) for k, v in Q_sarsa.items())\n",
    "plot_values(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotear la politica\n",
    "policy_sarsa = dict((k,np.argmax(v)) for k, v in Q_sarsa.items())\n",
    "plot_policy(policy_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q_sarsamax(alpha, gamma, Q, state, action, reward, next_state=None):\n",
    "    # Completar\n",
    "    return new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, alpha, gamma=1.0,epsmin=0.01):\n",
    "    # Completar\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la politica optima y el value function\n",
    "Q_sarsamax = q_learning(env, 500000, 0.01)\n",
    "V = dict((k,np.max(v)) for k, v in Q_sarsamax.items())\n",
    "plot_values(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotear la politica\n",
    "policy_sarsamax = dict((k,np.argmax(v)) for k, v in Q_sarsamax.items())\n",
    "plot_policy(policy_sarsamax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "245.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
